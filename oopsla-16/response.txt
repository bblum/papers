We agree with reviewer A about possible bias in the benchmarks. To address
this, we will add some analysis of the types of bugs found. For example, in
the Iterative Context Bounding control experiment, 82 bugs were found with
preemption bound 1, 15 with bound 2, and 2 with bound 3, consistent
with the distribution in [Musuvathi PLDI07]. This shows no bias towards
requiring high preemption bounds.

Maximizing completed state spaces is motivated by [Ball TACAS10], in which
developers prefer compositional testing (manually restricting a test's scope
to complete faster), which suggests that full completions of partial state
spaces are easier for humans to interpret than partial completions of full
spaces. We also showed subset state spaces generally uncover
bugs faster (QS-Sync-Only vs SSS-MC-DPOR). However, your suggestion to
sometimes complete larger jobs first, to prune subsumed smaller jobs, is an
insightful optimization for future work.

Reviewer B: We choose new preemption points to add for each
data-race candidate, one for each racing instruction. This produces several
new jobs, each with a different combination of the new ones with existing
ones. For the sake of soundness, which assumes an infinite CPU budget, it
would suffice to randomly choose among new jobs until all are exhausted, or
even to always choose the biggest (maximal) job and ignore subset jobs.
Section 5's proofs assume this simple approach (call it Algorithm 0).
Algorithm 1 in section 3 optimizes algorithm 0 for finite CPU budgets,
choosing the state space with smallest ETA. We will clarify this, including
Algorithm 0 explicitly.

Regarding the suggestion of Maximal Causality Reduction [Huang PLDI15], we
were not able to do a direct comparison because that paper has no open-source
implementation. This is a compelling direction for future work.

We will clarify in our introduction that we assume sequentially-consistent
hardware. Future work could extend our implementation and proofs using the
techniques in [Zhang PLDI15].

Figure 2 is meant to show the state of an arbitrary in-progress search,
although we agree it needs work. Regarding Figure 3, the contrived program is
just an example to explain the background. We agree other approaches exist
which can solve this case. Thanks for the suggestion about graph search; we
will look into it.

Reviewer C insightfully observes that Figure 7's count of bugs excludes
data-race candidates from the control experiments. This is the mentioned
philosophical difference about data races versus concrete failures; to address
this, we will add a table listing how many data races the control experiments
reported, and how many of those Quicksand refuted as false positives.

Reviewer D mentions that both HB and Limited HB will find all data races when
combined with model checking. We suspected this was true, but have only proven
the case for Limited HB. Could you provide a citation for us to include during
revisions? One further advantage of Limited HB is reporting candidates sooner,
allowing Quicksand to test them immediately, at the expense of more false
positives. We will clarify this trade-off.

Regarding our malloc-recycle solution, we tried your never reallocating
memory, but this unacceptably impacted malloc-heavy tests. Our proof allows
eliminating these data-race candidates without hacking the allocator.

Thanks for the clarification about FastTrack and CHESS; we will improve our
discussion of those tools.

Finally we agree with reviewers A and D and will work to improve the
formalism's notation and definitions. For example, in the definition of MHB,
we define "cannot be reordered" in terms of the latter thread's ability to be
scheduled at all intervening preemption points, with no intermediate blocking
operation (this coincides with DPOR's approach to identify alternate
interleavings to explore).
