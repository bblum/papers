\section{Evaluation}
\label{sec:eval}

Although \quicksand~presents Iterative Deepening and data-race PPs as interconnected techniques, they each could be employed alone as well. %in other model checkers.
For example, a single-state-space tool could use data-race PPs during immediately subsequent interleavings,
%essentially
changing the state space on the fly.
Likewise, a message-passing-only tool could use Iterative Deepening despite a concurrency model lacking data races, % being absent from its concurrency model.
to promote completing subset PP sets for large tests.
Hence, though many of our experiments compare \quicksand~to the state-of-the-art as a whole,
we also evaluated each technique individually.
Our evaluation answers the following questions:
\begin{enumerate}

	\item Does \quicksand~improve upon state-of-the-art MC?
		\begin{enumerate}
			\revision{\item} Do data-race PPs expose new bugs that couldn't be found with SSS-MC's fixed-PP-set approach?
				% Elaborate later:
				% Among those, how many were missed in a {\em completed} execution of the otherwise ``maximal'' state space?
			\revision{\item} Does Iterative Deepening find bugs faster
				%than SSS-MC
				in subset state spaces, even without data-race PPs?
			% Probably not... ICB is state of the art here.
			% In large tests, can Iterative Deepening provide partial verification by completing smaller state spaces
			\revision{
			\item Does Iterative Deepening provide more full verifications of bug-free programs more quickly?
			}
		\end{enumerate}
	\item Does MC improve the accuracy of data-race detection?
		\begin{enumerate}
			\item Do we avoid false positives compared to a single-execution data race analysis?
				% Explain later as:
				% How many data-race candidates were verified as benign
				% But to be fair, you have to count how many DRs are reported as "couldn't test these, check yourself" at the end.
				% Also Include:
				% How many false positives does the free-re-malloc technique suppress?
				% to do (done) If you have time, re-run all of the dr-only bug tests, with DR_FALSE_NEG enabled, and see how much fewer bugs get found (how many bugs get pushed past the time limit?)
				% Answer: Just 1. (for p2s at least; as of time of writing, pintos dr-falsenegs not run yet)
			\item Do we find data-race bugs that would be false-negatives during a single-execution analysis?%Do we avoid false negatives compared to single-pass?
		\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test Suite}
\label{sec:testsuite}
Our test suite consists of \numthrlibs~``P2'' student thread libraries, from Carnegie Mellon's 15-410 operating systems class,
and \numpintoses~``Pintos'' student kernels, from Berkeley's CS162 and University of Chicago's
\revision{CMSC 23000}~operating systems classes.
%
The P2 project comprises \texttt{thr\_create()}, \texttt{thr\_exit()}, \texttt{thr\_join()}, mutexes, condition variables, semaphores, and reader/writer locks;
all implemented from scratch in userspace with a UNIX-like system call interface \cite{kspec,thrlib}.
%
The Pintos kernel project
comprises priority scheduling, \texttt{sleep()}, and user-space process management (\texttt{wait()} and \texttt{exit()})
using provided mutex, context-switch, and virtual memory implementations
\cite{pintos}.
% P2 SLOC stats: 1807 avg; 1723 median; range 1181-4114.
% All numbers, obtained with:
% cd p2s; for i in */*; do wc -l $i/user/libthread/*.{c,h,S} $i/user/libthread/*/*.{c,h,S} ; done | grep total
% 1181 1192 1221 1230 1238 1240 1243 1261 1275 1307 1310 1318 1325 1334 1336 1345 1366
% 1388 1388 1403 1415 1416 1430 1451 1478 1498 1527 1589 1618 1635 1638 1654 1675 1676
% 1716 1719 1720 1723 1723 1727 1737 1743 1744 1751 1769 1777 1782 1789 1789 1812 1918
% 1926 1946 1994 2022 2043 2066 2077 2088 2099 2131 2164 2172 2190 2215 2227 2277 2282
% 2384 2387 2483 2486 2503 2514 2551 2597 2610 2665 4114
%Though not ``real world'' programs,
Both projects are quite complex:
the P2s average 1807 lines of code, %C and x86 assembly (stddev 489.5),
% Pintos SLOC stats: 718.1923077 avg; 643 median; range 68-1821
% Obtained with:
%function getloc {
%	diff -ru /tmp/shit/pintoses/di/src/$2/ $1/src/$2 | diffstat | grep insertions | sed 's/.*changed, //' | sed 's/ insertion.*//'
%}
%for i in `ls -d /tmp/shit/pintoses/* | grep -v -- "-p1$"`; do
%#for i in /tmp/shit/pintoses/daniel-deniz*; do
%	echo -ne "$i";
%	tloc=`getloc $i threads`
%	uloc=`getloc $i userprog`
%	dloc=`getloc $i devices`
%	echo -e "\t$tloc\t$uloc\t$dloc"
%done
and the Pintoses average 718 lines, for a total of 198,772.

We chose P2s and Pintoses for our test suite because of the relative ease of generating hundreds of unique state spaces,
varied in size and correctness, and with a diverse set of bug types\footnote{
Many of the codebases exhibited {\em deterministic} bugs (i.e., encountered on the first interleaving tested).
We fixed these by hand, before running tests, to ensure that every bug in our study required meaningful work by the MC.}.
% TODO: Is this ok? Too arrogant?
We believe that merely finding a small handful of new real-world bugs is largely anecdotal,
and that our test suite's size allows for a more statistically significant comparison among MC and data-race testing strategies.
%compared to anecdotally showing a small handful of new bugs that could be found in real-world programs.

\newcommand\mxtest{\texttt{mx\_test}}
\newcommand\tej{\texttt{join\_test}}
\newcommand\bct{\texttt{bcast\_test}}
\newcommand\paraguay{\texttt{signal\_test}}
\newcommand\paradise{\texttt{sem\_test}}
\newcommand\rwldgr{\texttt{rwlock\_test}}
\newcommand\prisema{\texttt{sched\_test}}
\newcommand\waitsimple{\texttt{wait\_test}}
\newcommand\alarmsimul{\texttt{alarm\_test}}

We tested P2s with 6 multithreaded programs:
% from the 410 test suite % XXX: I would like to say this but this is a lie; figure out what else i can say instead
% each tailored to exercise a different part of the P2 project
\mxtest, for locking algorithm correctness,
\tej, a test of thread lifecycle,
\bct~and \paraguay~for cvars,
\paradise~for semaphores,
and \rwldgr~for r/w locks.
For \mxtest, \paradise, and \paraguay, we used the {\tt without\_function} command to blacklist {\tt thr\_create}, {\tt thr\_exit}, and {\tt thr\_join},
and for \mxtest~we enabled \landslide's mutex-testing option
(see \sect{\ref{sec:landslide}}).
We tested Pintoses with 3 programs from the class's provided test suite: \prisema, a test of the kernel scheduling algorithm,
\alarmsimul, for the timer sleep routine,
and \waitsimple, for process lifecycle system calls\footnote{
	Some of the Pintoses were partially implemented,
	so each test could only be run on a subset of the 78 submissions; see ``Number tested'' in Table~\ref{tab:drbugs}.
}.
The source code of all 9 test cases is available at
{\em [submitted as supplementary material]}.
For all tests, we used {\tt without\_function} to blacklist PPs on {\tt malloc}'s internal lock.
In total, our evaluation comprises 629 unique tests (pairs of a test program and a Pintos or P2),
% at least 157 of which could expose bugs.
at least 165 of which could expose bugs. % QS bugs + bugs where SSSMC found but QS didnt
% TODO: update this number with any new bugs found by PREEMPT_EVERYWHERE

%\begin{table}[t]
%	\begin{tabular}{l|l|l}
%			& QS bugs & SSS-MC bugs \\
%		\hline
%		\mxtest & eg 1000 & eg 0 \\
%		\bct & & \\
%		etc... & & \\
%		\hline
%		Total & & \\
%	\end{tabular}
%	\caption{Comparison of all bugs found, broken down by test case, among all P2s (top 6) and Pintoses (bottom 2)}
%	\label{tab:allbugs}
%\end{table}
%
%\begin{table}[t]
%	\small
%	\begin{tabular}{l|l|l||l|l}
%	& QS bug & \begin{tabular}{c} SSS-MC \\ completed\end{tabular}
%	& QS bug & \begin{tabular}{c}SSS-MC \\ timeout \end{tabular} \\
%		\hline
%		\mxtest & e.g. 5 & 10 & 0 & 0 \\
%		\bct & & & & \\
%		etc... & & & & \\
%		\hline
%		Total & & & & \\
%	\end{tabular}
%	\caption{Bugs requiring data-race PPs to expose, found by \quicksand~but missed by the single-state-space approach.}
%	\label{tab:drbugs}
%\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\revision{
\subsection{Experimental Setup}

To evaluate the benefits of data-race PPs and Iterative Deepening separately, we ran the test suite under \quicksand~in two different configurations, each of which was given a 1-hour budget and 10 CPUs for each test.
\begin{itemize}
	\item {\bf QS-DR}: \quicksand~with data-race PPs enabled, and
	\item {\bf QS-Sync-Only}: \quicksand~with PPs still seeded as described in \sect{\ref{sec:initial-pp}}, but never adding new PPs from reported data-race candidates.
\end{itemize}

We represented the MC state-of-the-art with 3
%different
configurations of stand-alone \landslide~on the same test suite:
\begin{itemize}
	\item {\bf SSS-MC-DPOR}: Single state space using the maximal PP set from \sect{\ref{sec:initial-pp}}, explored with plain DPOR \cite{dpor},
	\item {\bf SSS-MC-ICB}: With PPs as above, but instead using ICB \cite{chess-icb} with BPOR \cite{bpor} to find bugs faster, and
	\item {\bf SSS-MC-Shared-Mem}: % TODO: better name?
		Using ICB+BPOR, configured to insert PPs on any shared memory access
		(determined at runtime, excluding threads' accesses to their own stacks),
		which in principle includes all possible data-race PPs.
\end{itemize}
Because parallelizing DPOR/ICB during SSS-MC is an open research problem \cite{parallel-dpor},
we instead gave control experiments 10 hours per test with 1 CPU.
%To match this,
\quicksand~reports the CPU-time spent alongside the wall-clock time for a direct comparison
(although, with the growing importance of multicore, \quicksand's inherent parallelism is a convenient benefit).
%Finally,
All tests ran on 12-core 3.2 GHz Xeon W3670 machines. %with 12GB of RAM.

\subsection{\revision{Comparison to state-of-the-art MC}}%Comparing Iterative Deepening to SSS-MC
\label{sec:eval-sssmc}

Figure~\ref{fig:dowefindbugsfaster} plots the cumulative distribution of bugs found \revision{by each experiment}.
\revision{
Figure~\ref{fig:dowefindbugsfaster}(a) is the main comparison by CPU-time;
we additionally show a wall-clock comparison in (b) to highlight the impact of \quicksand's parallelism.
}

\begin{figure}[t]
	\hspace{-1em}
		\revision{
	\begin{tabular}{p{0.48\textwidth}}
	\includegraphics[width=0.48\textwidth]{dowefindbugsfaster-squashed.pdf} \\
		(a) Bugs found by elapsed CPU time. Overall, a more direct comparison than (b),
		although \quicksand's start-up overhead is exaggerated, as the SSS-MC tests are not parallelized. \\
		\\
	\includegraphics[width=0.48\textwidth]{dowefindbugsfaster-wallclock.pdf} \\
		(b) Bugs found by elapsed wall-clock time.
		\quicksand~is parallelized tenfold; the vertical line indicates its 1 hour limit. \\
	\end{tabular}
	}
	\caption{Comparison of bug-finding performance
	by several configurations of \quicksand~and the SSS-MC control.
	\quicksand~finds \revision{151}\% as many bugs with data-race PPs.}
	\label{fig:dowefindbugsfaster}
\end{figure}

{\bf Finding new data-race bugs.}
%We quickly pull ahead of SSS-MC, and ultimately conclude with 179\% as many bugs in total.
%The break-even point is at a negligible 90 seconds.
\revision{Compared to the best SSS-MC series, \quicksand~finds}~more bugs within any CPU budget greater than \revision{200}~seconds,
and ultimately concludes 10 CPU-hours with \revision{151}\% as many bugs in total.
\revision{Before the break-even point at 200 seconds,
% TODO
% (for SSS-MC-ICB) or XXX seconds (for SSS-MC-Shared-Mem)
\quicksand~lags behind the SSS-MC approaches due to additional start-up overhead from its tenfold parallelism.
However, converting SSS-MC's early CPU-time advantage into faster wall-clock performance remains an open research problem \cite{parallel-dpor}.
In Figure~\ref{fig:dowefindbugsfaster}(b), we give \quicksand~full credit for its inherent parallelism, and it outperforms SSS-MC for any fixed budget of wall-clock time.}

%{\bf Variance.}
% TODO CAMREADY: Probably will need at least a 3rd run of dis.
Because \quicksand~can be nondeterministic in its scheduling of state spaces,
we ran the QS-DR tests a second time to quantify the variance.
We found the area between the two QS-DR curves to be
% TODO
{\bf XX\%}
of the area between QS-DR and SSS-MC-ICB (the next closest),
% TODO cite
which we consider an insignificant amount of variance.
}


% TODO: Fix up this paragraph after preempt-everywhere results come in.
The left half of Table~\ref{tab:drbugs}
breaks down the number and types of bugs found by each test program.
In \mxtest, in which we do not trust the lock implementation's correctness,
the control experiment found dramatically fewer bugs (just 1)\footnote{
	The one bug SSS-MC found was in a fully-assembler lock implementation. {\tt yield()}'s return value clobbered a value stored in {\tt \%eax}, which could lead to a failure after two repeated contentions. Preempting only on {\tt yield()} (in the contention loop) was sufficient to find the bug.}.
%Intuitively, this is due to our control experiment being able to preempt only on the boundaries of the API which
%Though for many applications of MC, assuming a correct lock implementation is sufficient,
Though it often suffices to assume correctly-implemented locks,
we consider this strong evidence that new low-level synchronization code must be verified with data-race PPs.
%TODO CAMREADY: Run a mutex expt where "all atomic instrs" are PPs. See how many bugs are missed anyway.
% Wwe re-ran the \mxtest control experiment with \landslide~hard-coded to preempt on any atomic instruction
% (as well as on the mutex API boundaries).
% Still, this smarter configuration for SSS-MC found only 99999999999 bugs of \quicksand's 13.

%% RIP this.
%Furthermore, we plotted another line from this dataset, QS-no-DR-bugs,
%which represents only the bugs found in state spaces without any data-race PPs (like QS-no-DR-PPs, but paying any overhead for using data-race PPs).
%Intuitively, this line shows that for programs with only benign data races,
%\quicksand~can afford the extra overhead of verifying them while still slightly edging out SSS-MC.

% TODO: fix this table.
\begin{table*}[t]
	\begin{center}
		\small
	\begin{tabular}{r|c||c|c|c|c|c||c|c||c|c}
		% TODO: Update SSSMC column to have more favourable ICB numbers.
		& {\bf Num.} & {\bf SSS-MC} & {\bf QS} & {\bf DR-only} & {\bf Nondet.} & {\bf Malloc-} &
		{\bf Mutual} & {\bf Avg. tested} & {\bf Total} & {\bf Untested} \\
		{\bf Test name} & {\bf tested} & {\bf bugs} & {\bf bugs} & {\bf bugs} & {\bf DR bugs} & {\bf recycle DRs} &
		{\bf time-outs} & {\bf subset SSes} & {\bf DR PPs} & {\bf DR PPs} \\
		\hline
		% FIXME: update FRMs
				% #test  sssmc   qs      dronly  nondets  FRMs   timeout comp.SSes DRPPs untested DRPPs
		{\tt bcast}	& 79	& 7	& 11	& 4	& 4	& 51	& 0	& -	& 912	& 107	\\
		{\tt join} 	& 79	& 14 	& 24	& 10	& 6	& 333	& 5	& 60.8	& 781	& 292	\\
		{\tt mx} 	& 79	& 1	& 10	& 9	& 1	& 7	& 0	& -	& 829	& 1	\\
		{\tt sem} 	& 79	& 10	& 16	& 6	& 5	& 140	& 29	& 73.3	& 753	& 279	\\
		{\tt signal} 	& 79	& 4	& 10	& 7	& 6	& 180	& 10	& 54.2	& 1118	& 391	\\
		{\tt rwlock} 	& 79	& 26	& 28	& 2	& 2	& 125	& 19	& 29.6	& 915	& 634	\\
		\hline
		{\tt sched} 	& 59	& 1	& 7	& 6	& 4	& 0	& 0	& -	& 144	& 3	\\
		{\tt alarm} 	& 44	& 5	& 21	& 17	& 3	& 35 	& 22	& 9.1	& 115	& 89	\\
		{\tt wait} 	& 52	& 23	& 30	& 7	& 1	& 31	& 8	& 28.0	& 142	& 31	\\
		\hline
		{\bf Total}	& 629	& 91	& 157	& 68	& 32	& 902	& 93	& 42.6 	& 5709	& 1827	\\
	\end{tabular}
	\end{center}
	\caption{Summary of bugs and data races found by each test program.
		SSS-MC is the control; QS is \quicksand.
		``DR-only bugs'' counts among \quicksand's bugs how many required data-race PPs to expose (\sect{\ref{sec:eval-sssmc}});
	among those, ``Nondeterministic DR bugs'' counts how many candidates required MC integration to identify (\sect{\ref{sec:eval-dr}}).
	``Malloc-recycle DRs'' counts how many false positives we suppressed (\sect{\ref{sec:eval-dr}}).
	``Mutual time-outs'' counts how often both SSS-MC and \quicksand~timed out with no bug found;
	among those, ``Average tested subset SSes'' counts how many partial verifications \quicksand~provided on average for each test (\sect{\ref{sec:eval-sssmc}}).
	``Total DR PPs'' counts how many unique data-racing instructions we identified among tests where we found no bugs;
	among those, ``Untested DR PPs'' counts how many could not be checked in the time limit (\sect{\ref{sec:future}}).
		}
	\label{tab:drbugs}
\end{table*}

% TODO CAMREADY: Compare QS-ETAs to QS-Random to evaluate "smaller is better" claim.
% TODO: Nix discussion of QS-no-DR-bugs. Just do QS-no-DR-PPs.
{\bf Finding the same bugs faster.}
%To test whether Iterative Deepening is effective even for MC domains without data races, %such as message-passing distributed systems,
\revision{The QS-Sync-Only experiment tests whether}~Iterative Deepening is effective even for MC domains without data races.
\revision{When \quicksand~ignores all data-race candidates,
its results are competitive with SSS-MC-DPOR, but SSS-MC-ICB outperforms it.
This is unsurprising: the seed subsets of PPs (\sect{\ref{sec:initial-pp}}) QS-Sync-Only is limited to are much less flexible than ICB's preemption strategy.
This result suggests that in future work, \quicksand~should consider using ICB in parallel with its default configuration when it finds no data-race candidates to test.}
%\footnote{
%Because \quicksand~is not yet instrumented to subset hard-coded PPs beyond the 4 ways shown in Figure~\ref{fig:id},
%we ran these tests for 2.5 hours on 4 CPUs each.
%Future work could parallelize QS-no-DR-PPs further; see \sect{\ref{sec:future}}.}.

%Even though SSS-MC mostly catches up to it by the end of the 10-hour budget,
%and is faster in the first 60 seconds due to less start-up overhead,
%QS-no-DR-PPs finds more of the bugs sooner thereafter.
%Hence, for modest CPU budgets,
%\quicksand~is likely to find bugs SSS-MC will miss.
%and for more ambitiously-sized tests,
%programmers can be more confident in the verification provided when \quicksand~times out with no bug found.
%We conclude that for smaller arbitrary CPU budgets, especially less than 1 hour,
%Iterative Deepening is likely to find bugs SSS-MC will miss.
%Moreover, it is also easy to imagine scaling up the size of each test case to test ,
%using more threads or longer sequences of API calls.
%We hope that is compelling even to users willing to spend many CPU-hours on testing.
%These results show explicitly that for arbitrary CPU-time budgets
%\footnote{The initial perfect overlap between QS-DRs and QS-no-DR-bugs indicates how long it takes before the first data-race bug is found.}
%even after the extra overhead of verifying them, \quicksand~still slightly edges out SSS-MC

\revision{
On the other hand,
comparing QS-DR to SSS-MC-Shared-Mem shows that Iterative Deepening thoroughly outperforms ICB when shared-memory preemptions come into play.
%We attribute this to the fact that
Statically configuring a PP for every shared memory access in advance
produces orders of magnitude more PPs than
waiting for an access to be identified as part of a (potential) data race at runtime.
%
In principle, DPOR and BPOR should identify and prune any equivalences arising from extraneous PPs on non-conflicting accesses.
However, in practice,
%we found that
the sheer number of accesses during each new execution (often thousands) added significant performance overhead to the MC when computing DPOR and backtracking\footnote{
	\revision{The bug-finding performance of SSS-MC-ICB approach could be improved by heuristically
	%preempting only on data-race candidates found in advance by a single-pass analysis
	using a single-pass analysis to find data-race PP candidates in advance
	\cite{portend}, but this sacrifices soundness, as shown in \sect{\ref{sec:eval-dr}}.}
}.
Iterative Deepening avoids this overhead by waiting until runtime to identify fewer, more relevant PPs dynamically,
and is hence more suitable for MC with data-race PPs.}

{\bf Partial verification.}
When a MC job times out, the user may prefer a brief summary of what parts of the test were verified, rather than writing off all the CPU time as a waste.
%Beyond using the state space estimator to guess the percent coverage,
We know of no approach to quantify the probability
that a bug would be exposed by an untested interleaving,
but \quicksand~at least reports which subsets of PPs resulted in state spaces that did complete in time.
%\quicksand~does not attempt any such quantitative guarantee,
%but can at least report
On 129 tests, the SSS-MC control experiment timed out after 10 hours with no bugs found.
Among these tests, \quicksand~found bugs in 36.
For the other 93, we show the number of state spaces \quicksand~was able to complete in the ``Average tested subset SSes'' column of Table~\ref{tab:drbugs}.
%and the number of unique PPs among them.
%In 6 cases, \quicksand~also failed to complete anything; beyond these,
%between 1 and 253 state spaces were completed for each test
%Between 0 and 233 unique PPs were tested among some completed subsets, with a mean of 27.2 and median of 7.5.
These completions guarantee that, if the test program could expose a bug,
% Justify implicit hypothesis: Sync PPs + DR PPs = all possible relevant PPs.
it would only be found by a new data-race PP not discovered yet, or by a superset combination of PPs not reached.

%Hence, in Table~\ref{tab:drbugs} we count how often \quicksand~uncovered a bug only in state spaces which included data-race PPs, while
%
%In Table~\ref{tab:allbugs} ....

{\bf Full verification.}
% TODO: Discuss how ICB sucks even more.
For 153 of our 629 tests, \quicksand~was able to provide the total verification guarantee described in \sect{\ref{sec:totalverif}}.
In Figure~\ref{fig:totalverif} we plot the cumulative distribution of \quicksand's total verifications
%before a given elapsed CPU consumption.
\revision{against those provided by SSS-MC-Shared-Mem\footnote{
	\revision{In kernel or thread library code, a thread's own stack accesses may not always be private. %assumption may not hold.
	For example, many P2 condvars use stack-allocated list nodes to track waiting threads.
	SSS-MC-Shared-Mem's verifications are only sound in the absence of such sharing!
	\quicksand~detects these dynamically, without needing to discriminate between stack and heap.}
	%Expanding the statically-coded PPs to include stack accesses would make SSS-MC-Shared-Mem's performance even worse.
	% Especially for testing kernel-level code, deciding which of a thread's memory accesses are shared versus private is nonstraightforward, as threads may share data structures allocated on their own stacks for synchronization. Hence, the common heuristic of ignoring stack accesses as private (which we employ for this control experiment) introduces some unsoundndess.
}.
\quicksand~outperforms SSS-MC, being able to verify safety for
% TODO
{\bf 99x}
as many tests in 10 CPU-hours.
We attribute this again to the overhead of the sheer number of statically-coded PPs that SSS-MC-Shared-Mem must use,
as well as to the work which ICB necessarily repeats when increasing its preemption bound.}

Among these verified tests, 36 contained no data-race candidates whatsoever,
so the same verification could be provided
%by SSS-MC
\revision{with synchronization PPs only.
We plot the verifications by QS-Sync-Only, SSS-MC-DPOR and SSS-MC-ICB as well:
among these, the otherwise more antiquated SSS-MC-DPOR performs best,
while the other two lag behind
}
%\quicksand~is slower than SSS-MC-DPOR
due to redundant work (\sect{\ref{sec:future}}).
\revision{While QS-Sync-Only and SSS-MC-ICB are competitive with each other},
using data-race PPs increases our \revision{verification capacity}~by 4.25x.

\begin{figure}[t]
	\includegraphics[width=0.48\textwidth]{totalverifs-squashed.pdf}
	\caption{Cumulative distribution of tests \quicksand~fully verified (\sect{\ref{sec:totalverif}}).
	Some tests had no data-race candidates, and hence could also be verified by SSS-MC.}
	\label{fig:totalverif}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparing to single-pass data-race analysis}
\label{sec:eval-dr}
% Though we mechanically verify whether each data race candidate leads to a bug, each new PP can increase combinatorially..... obviously wish to avoid...

Beyond finding new bugs with data-race PPs, we evaluated \quicksand's performance for classifying data-race candidates in two ways.

{\bf Suppressing ``malloc-recycle'' false positives.}
In \sect{\ref{sec:recycle}} we showed the soundness of suppressing data race reports between two heap accesses when the surrounding memory was re-allocated in between.
In Table~\ref{tab:drbugs}, the column ``Malloc-recycle DRs'' shows the total number of such data-race candidates for each test program.
In total, 902 data-races fit the malloc-recycle pattern across all tests,
only 69 of which were observed to avoid the re-allocation in an alternate interleaving.
Our proof in \sect{\ref{sec:recycle}} guarantees the safety of pruning all 833 other state spaces.

Among those 69 true data-races, %which initially fit the malloc-recycle pattern,
none exposed a new bug when used as a PP.
This suggests that for other data-race tools,
suppressing malloc-recycle candidates may be a productive heuristic,
even if unsound without Iterative Deepening.
However, \quicksand~was able to correctly identify the 69 violations of that heuristic (among 30 distinct tests),
and fall back to classifying them with DPOR.
%which we consider a much stronger verification.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%For the [NONDET] expt, you don't need to additionally count how many DRs
%needed to be found in a DR-PP state space, because when EXPLORE_BACKWARDS=0,
%you'll need to preempt on the DR PP to find the new DR anyway, hence they will
%be nondet. Unless you want to separately count the subset of nondet drs that
%are also dr-pp-ss-only (the reviewers may ask for this before cam-ready?).
{\bf \revision{Finding nondeterministic data-race candidates}.}
Some memory accesses may be hidden in a control flow path that requires a nondeterministic preemption to be executed.
In such cases, a single-pass dynamic data-race detector
%could not achieve the coverage necessary
could fail
to identify a racing access pair as a candidate to begin with.
%
We counted how many such data-races, used as PPs, led to \quicksand~finding new bugs,
thereby making them {\em false negatives} of the single-pass approach.
We classified each data-race candidate according to whether \landslide~reported them during the first interleaving,
before any backtracking or preempting:
if so, they were {\em single-pass data races}; otherwise, {\em nondeterministic}.

To ensure a fair comparison, we disabled \landslide's {\em false-positive}-avoidance techniques during this experiment.
For example, we reported malloc-recycle data races during the first interleaving, as a single-pass analysis must
%rather than waiting until future interleavings to witness them without the malloc-recycle pattern
(\sect{\ref{sec:recycle}}).
This prevents \landslide~from suppressing an observed data race on the first interleaving,
which would falsely classify it as nondeterministic.

\begin{figure}[t]
	\includegraphics[width=0.48\textwidth]{nondet-drs-1-v2.pdf}
	\caption{Some data-race candidates may not be identified during a single program execution.
		Using nondeterministic data races as PPs,
		\quicksand~found 189\% as many data-race bugs compared to using single-pass candidates alone.
	}
	\label{fig:dr-falsenegs}
\end{figure}
Figure~\ref{fig:dr-falsenegs} compares the types of data-race candidates necessary to expose each data-race bug in our test suite.
The first series represents the bugs found using PPs from single-pass data-race candidates,
% not entirely true, as portend could be given data-race traces from an MC,
%but they don't do it in their paper, so i feel comfortable making this claim
i.e., the state-of-the-art approach used by \cite{racefuzzer,portend}.
The second series shows all data-race bugs \quicksand~found,
which includes the former type as well as new bugs involving nondeterministic data-races.
\quicksand~found 68 data-race bugs in total, only 36 of which could be found with single-pass data-race candidates alone.
%In total, we found 189\% as many bugs by using nondeterministic data-race candidates as PPs.

%When proving that [NONDET] dr buges exist, make sure to mention that, although
%their frequency varies depending on test case (mx test: almost none; bct:
%almost all), they are still PRESENT in all (or almost all) test cases, meaning
%it is not just a matter of writing better test cases.

Note that we are not comparing how much testing time is required before identifying the data-race candidates involved in each bug.
Single-pass data races can all be found after a single program execution,
while \quicksand~may potentially take up to all 10 CPU-hours before identifying a nondeterministic data race.
However, prior work data-race tools \cite{tsan}, being not integrated with a MC,
are not intended to discover new candidates under subsequent runs.
Running a single-pass data-race tool repeatedly for 10 CPU-hours could potentially uncover some nondeterministic candidates,
but stress testing's comparative problem with achieving reliable coverage is already well-understood
\cite{chess-icb,gambit}.
Likewise, replay-based tools \cite{portend} are dependent upon the data-race detector to provide an execution trace leading to each candidate.
This experiment suggests that
such tools could benefit from a similar feedback loop as used in Iterative Deepening.
%i.e., discovering transitively-reachable data races while testing initial ones.
%although that would still not simultaneously be able to provide total verifications.
% TODO: read gambit paper


% Figure out concretely what the data race tricks are that we do, so we can claim them as contributions in the paper. Then ACTUALLY EVALUATE THEM.
%         - Speculative DR PPs.
%                 Not a heuristic, rather how to make it work at all to begin with.
%                 (Cite MS thesis, claim on backwards explorating finding bugs faster)
%         - Free/re-malloc to eliminate some false positives. See #193.
%                 Measure how many false positives are eliminated.
%                 Check, ofc, to make ABSOLUTE SURE, that no bugs missed w/ this trick.
%                         If there are, it could be because of the implementation
%                         bug described in #193.
%         - Using tid/last_call filtering because whole stack traces are too expensive.
%                 Moderately optional, 1st priority since theoretically interesting:
%                 Turn on/off and measure how resulting DR bug #s change.
%         - Optional: Reprioritizing DRs based on "confirmed" / "suspected"
%                 Shouldn't be hard just make ID wrapper print "s" or "c"!
%                 Is it helpful for ID to put priorities on DR PPs?
%                         Test by inverting the priority and see if fewer buges are found.
%         // Super optional to talk about. Probably not worth the time.
%         // - "Too suspicious" (during init/destroy)
%         //      (Cite eraser, section 2.2)
