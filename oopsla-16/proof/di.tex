\section{Definitions}

\subsection{System Model}

To avoid relying on any particular programming language features,
we leave the program syntax and execution semantics opaque,
reasoning instead about execution traces.
We require that a program's evaluation produce a trace of instructions of the following form:
\begin{eqnarray*}
	\mathcal{A} &::=& v \leftarrow \mathsf{read}(a) \quad | \quad \mathsf{write}(a,v) \quad | \quad \mathsf{xchg}(a,v) \\
		&|& a \leftarrow \mathsf{malloc}(n) \quad | \quad \mathsf{free}(a) \\
		&|& \mathcal{A}_{local} \quad | \quad \mathcal{A}_{sync}
\end{eqnarray*}
The execution steps take the following forms:

{\bf Memory.}
$\mathsf{read}$, $\mathsf{write}$, and $\mathsf{xchg}$ access global or heap memory shared by all threads, indicated by some address $a$.
(Other atomic swap operations are omitted for brevity.)
$\mathsf{malloc}$ and $\mathsf{free}$ provide access to fresh memory accessible by all threads. % free is irrelevant to the system model in terms of expressive power

{\bf Local state.}
$\mathcal{A}_{local}$ represents any thread-local instruction, such as modifying local variables, flow control, function calls, and assertions. %, and side effects.
We omit a detailed list for brevity, as we do not need to reason about them in these proofs.
%For brevity, we omit a detailed list, although note that $\mathsf{call}$ would be implemented by modifying the instruction stream $[\mathcal{I}]$ and creating a new frame on the stack of local variables.

{\bf Threads.}
$\mathcal{A}_{sync}$ denotes the subclass of evaluation steps which implement inter-thread synchronization; i.e., the {\em synchronization API}:
\begin{eqnarray*}
	\mathcal{A}_{sync} &::=& \mathsf{mutex\_lock}(m) \quad | \quad \mathsf{mutex\_unlock}(m) \\
		&|& \mathsf{deschedule} \quad | \quad \mathsf{make\_runnable}(t) \\
		&|& t \leftarrow \mathsf{thread\_fork} \quad | \quad \mathsf{thread\_exit} \quad | \quad \mathsf{yield}
\end{eqnarray*}
$\mathsf{mutex\_lock}$ and
$\mathsf{mutex\_unlock}$ provide mutual exclusion:
a thread which evaluates $\mathsf{mutex\_lock}$ on some lock $m$ becomes blocked until no other thread holds $m$.
% the second formulation here excludes the possibility of m_r waking a thread blocked on mutex_lock (which would be horrible)
%$\mathsf{deschedule}$ and $\mathsf{make\_runnable}$ allow threads to manipulate their own or another's runnability, respectively,
$\mathsf{deschedule}$ alows a thread to block itself until another thread wakes it with $\mathsf{make\_runnable}$,
%
and $\mathsf{thread\_fork}$ and $\mathsf{thread\_exit}$ allow creation and destruction of new threads.
$\mathsf{thread\_fork}$ is defined in the Pebbles manner \cite{kspec};
we omit higher-level abstractions such as $\mathsf{cond\_wait}$, $\mathsf{create}$, or $\mathsf{join}$, which can be implemented using these primitives \cite{thrlib}.
%$\mathsf{yield}$ has no effect under these execution semantics, but is included for the sake of synchronization API preemption points (see below).
$\mathsf{yield}$
allows the execution semantics to switch threads %without blocking the invoker,
while respecting the preemption-point-switching invariant discussed below.
%would have no effect under an execution semantics which schedules threads arbitrarily
%but is included for the sake of synchronization API preemption points (see below).

\begin{definition}[Interleaving]
An interleaving (or execution trace) is a list of these instructions, annotated to indicate the currently-running thread as well as the runnability of all existing threads:
\begin{eqnarray*}
	\mathcal{I} &::=& [\mathcal{A}, t, (t \rightarrow \mathsf{bool})]
\end{eqnarray*}
\end{definition}

We say a {\em thread switch} occurs when adjacent elements in $\mathcal{I}$ have different thread IDs $t$.
%
We say a thread is {\em blocked} when its value in the runnability map is false.

{\bf Interleaving invariants.}
We require the evaluation semantics to produce interleavings which fulfil several invariants,
apart from the obvious ones ensuring correct synchronization and $\mathsf{malloc}$ discipline.

Model checkers often include heuristics to identify blocking via open-coded $\mathsf{yield}$ loops,
but we assume here that such patterns are implemented more tastefully with a condition-variable-like primitive built upon $\mathsf{deschedule}$.

Most importantly, we assume that threads switch only at instructions identified by the set of {\em preemption-point predicates}, defined below.

\subsection{Stateless model checking terms}

\begin{definition}[Preemption point (PP) predicate]
	A predicate on the execution state which identifies a class of instruction pairs between which we may force threads to switch.
\end{definition}

We use {\em synchronization API PPs} to denote the set of predicates
%statically-available
which occur immediately before or after any of $\mathcal{A}_{sync}$.
%$\mathsf{mutex\_lock}$,
%$\mathsf{mutex\_unlock}$,
%$\mathsf{deschedule}$,
%$\mathsf{make\_runnable}$,
%$\mathsf{thread\_fork}$,
%$\mathsf{thread\_exit}$,
%$\mathsf{yield}$.
Because no other instruction affects a thread's runnability, it is always possible to execute a program by switching the currently-executing thread only at synchronization API PPs.

All data-race PP predicates will occur immediately before a $\mathsf{read}$, $\mathsf{write}$, or atomic swap.
Other predicates are possible, though we will show they are irrelevant.

When generating execution traces, the evaluation semantics is parameterized by a set of active PP predicates.
As long as the set contains the synchronization API PPs,
%the execution shall switch threads only when a PP predicate holds.
the thread-switch invariant discussed above will hold.

\begin{definition}[Preemption point (PP)]
	%A PP is a code location between two instructions at which we may force threads to switch.
	%A PP is a predicate on the execution state which identifies a class of instruction pairs between which we may force threads to switch.
	A PP is any site in an interleaving at which a PP predicate evaluated to true.
\end{definition}

As discussed above, all thread switches occur at PPs.
However, an interleaving may also contain PPs at which the same thread continued running.

In the main paper, ``PP'' referred to what we now call ``PP predicates''.
Hence, the ``same'' PP could occur multiple times during an execution.
%for example, {\tt mutex\_lock()} may be called from {\tt foo()} and later again from {\tt bar()}.
In these proofs, we separate such cases into multiple unique PPs:
each PP is simply a label denoting the boundary between two transitions.

\begin{definition}[Transition]
A sequence of execution steps from a program's evaluation between two PPs.
\label{def:transition}

\end{definition}
The thread-switch invariant guarantees that each transition's instructions are associated with exactly one thread.
%The set of synchronization API PPs provides this invariant, and all other PP sets in these proofs will be supersets of those.
%We also assume a trace of all memory accesses is available in the representation of transitions. %% It is, now.

\begin{definition}[State space]
	A state space $\mathcal{S}$ is a set of interleavings representing all execution sequences legal under a given set of PP predicates.
\end{definition}

\begin{definition}[Must-happen-before (MHB)]
%Given two transitions $A$ and $B$, we say $A$ MHB $B$ if $B$ cannot be reordered to occur before $A$.
Let $t_1$ and $t_2$ be two transitions of an interleaving, and $T1$ and $T2$ be the corresponding thread IDs,
and let $t_1$ occur before $t_2$.
Then $t_1$ MHB $t_2$ if
\begin{enumerate}
	\item $T2$ is blocked immediately preceding $t_1$ and not blocked immediately afterward,
		and there does not occur another $t_2'$ by $T2$ between $t_1$ and $t_2$; or
	\item there occurs some $t_3$ by thread $T3$ such that $t_1$ MHB $t_3$, $t_3$ MHB $t_2$, and $T3 \ne T2$; or
	\item $T1 = T2$.
\end{enumerate}
\end{definition}

Intuitively, MHB expresses when two transitions cannot be reordered
(the ``enables'' relation in DPOR terminology \cite{dpor}).
Two transitions $A$ and $B$ of different threads MHB if some synchronization event in $A$ causes $B$ to become runnable while it was previously blocked.
Such synchronization events include $\mathsf{thread\_fork}$, $\mathsf{make\_runnable}$,
and sometimes but not always $\mathsf{mutex\_unlock}$.

Note how our {\em must}-happen-before relation differs from the conventional definition of happens-before (``observed to happen before'') \cite{lamport-clocks}.
Our use of MHB matches the ``limited happens-before'' used in \cite{hybriddatarace} and \cite{tsan};
the advantage of this over pure-happens-before detectors in producing fewer false negatives is well-argued in those prior works\footnote{
Because pure-HB data race detectors avoid false positives altogether, they would have no trouble avoiding our malloc-recycle false positives.
However, as prior work has shown, they miss many other bugs involving unprotected variables accessed alternately before and after mutex-protected critical sections.
%Indeed, because most concurrent malloc implementations are protected by a lock,
%our malloc-recycle false positives are indistinguishable from such false negatives under pure-HB.
}.
We illustrate the difference in Figure~\ref{fig:mhb}.

\begin{figure}[h]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & \texttt{\hilight{brickred}{my\_x->foo = ...;}} & \\
	2 & \texttt{\hilight{olivegreen}{mutex\_lock(...);}} &\\
	3 & \texttt{global\_x = my\_x;} & \\
	%4 & \texttt{\hilight{olivegreen}{yield();}} & \\
	4 & \texttt{\hilight{olivegreen}{mutex\_unlock(...);}} & \\
	5 & & \texttt{\hilight{olivegreen}{mutex\_lock(...);}} \\
	6 & & \texttt{my\_x = global\_x;} \\
	7 & & \texttt{\hilight{olivegreen}{mutex\_unlock(...);}} \\
	8 & & \texttt{if (my\_x != NULL)} \\
	9 & & \texttt{\hilight{brickred}{~~~~my\_x->foo = ...;}} \\
\end{tabular}
	\caption{Example program to illustrate the difference between {\em pure happens-before} and {\em must-happen-before}.
	Under pure happens-before (which does not identify false positives), lines 1 and 9 are not a data race candidate.
	Under MHB, they are; although after trying to reorder them, it will be classified as a false positive.}
	\label{fig:mhb}
\end{figure}

Note also that although transitions of the same thread are related by MHB,
MHB is transitive only when the latter two transitions are not by the same thread (condition 2).
While lock-protected critical sections can be reordered around each other (i.e., line 1 not MHB lines 8-9),
one cannot be reordered to be in the middle of the other (i.e, lines 3-4 MHB line 6).
%Hence, MHB is not necessarily transitive.
In the latter case, the MHB relation is established by the mutex's blocking mechanism used during contention.

Our main paper refers to this relation (in conjunction with a lock-set analysis) as Limited HB.

\begin{definition}[Shared memory conflict]
A pair of memory accesses between two threads to the same address where at least one of them is a write.
\end{definition}

% Outdated. See above.
%\begin{definition}[Interleaving]
%	An ordered list of transitions and preemption points between them.
%\end{definition}

\begin{definition}[Independent transitions]
Two transitions between different threads are independent if the intersection of their shared memory accesses contains no conflicts.
\end{definition}

\begin{definition}[Equivalent interleaving]
Two interleavings are equivalent if one can be transformed into the other by permuting only independent transitions.
\end{definition}

Intuitively, the behaviour of a program could change by reordering two transitions only if they contain a memory conflict.
All possible interleavings of a program can be partitioned into equivalence classes,
so only one interleaving from each equivalence class need be tested to ensure total schedule coverage \cite{mazurkiewicz}.
Equivalence is, of course, transitive.

% Outdated. See above.
%\begin{definition}[State space]
%	A set of interleavings subject to the constraint that, given the preemption points used, all equivalence classes of possible interleavings are represented by at least one member.
%\end{definition}

\begin{definition}[Dynamic Partial Order Reduction (DPOR)]
	A state-space search algorithm for stateless model checkers;
	given a state space $\mathcal{S}$, it will test at least one interleaving from each equivalence class in $\mathcal{S}$.
	%guaranteed to reorder transitions of two threads
	%iff they are not independent and are not related by MHB \cite{dpor}.
	\label{def:dpor}
\end{definition}

Considering an interleaving $\mathcal{I}$ in $\mathcal{S}$, if two transitions $t_1$ and $t_2$ by different threads are not independent and not related by MHB, let $\mathcal{J}$ be the interleaving which reorders $t_1$ with $t_2$. DPOR is then guaranteed to test some interleaving in $\mathcal{S}$ equivalent to $\mathcal{J}$ \cite{dpor}.

Because equivalent interleavings produce identical execution states,
DPOR guarantees to expose all reachable execution states by testing its subset of interleavings.
We refer to this property as {\em the soundness of DPOR}.

%The soundness of DPOR guarantees that if a program behaviour can possibly be exposed by any thread interleaving around the given transitions/PPs,
%that interleaving will eventually be tested by reordering only such conflicting transitions.
%In other words, reordering memory-independent thread transitions cannot possibly affect program behaviour.

\subsection{Data race and other memory terms}

\begin{definition}[Data race]
A shared memory conflict where furthermore:
\begin{itemize}
	\item The intersection of both threads' locksets is empty (i.e., the same lock does not protect both accesses), and
	\item The containing transitions are not related by MHB.
\end{itemize}
\end{definition}

The same as in the paper, we distinguish between data-race {\em candidates} (or {\em potential} data races) and data-race {\em bugs}.
For brevity, we now use ``data race'' to refer both to true races and to potential data-race candidates identified by MHB.
%In this proof we are concerned solely with candidates, and whether they can be observed to race or are false positives.
%It is up to the MC to decide whether true data races are benign or buggy.

\begin{definition}[False positive data race]
	An apparent data race that cannot be executed in the opposite order from what was observed.
\end{definition}

False positives are caused when some data dependency based on some other shared state %, invisible to the data-race analysis,
changes some variable values when the threads are reordered, such that the memory addresses no longer collide.

\begin{definition}[Malloc-recycle data race]
	A data race where the address is contained in some heap-allocated memory, and between the two accesses, that memory was passed to free() and returned again by a subsequent malloc().
\end{definition}

Figures~\ref{fig:recycle} and \ref{fig:recycle-bug} show an example.
In the case of malloc-recycle false positives, the allocation heap is the ``other shared state'' mentioned in the previous definition, and malloc's return value is the variable value that changed.

Recent work \cite{sparc-ssm} proposed hardware techniques for detecting many classes of stale heap pointer accesses, including the one shown in Figure \ref{fig:recycle-bug}.
Future work could combine this approach with MC to identify such bugs immediately,
rather than requiring Iterative Deepening to explore new state spaces corresponding to the data race.
However, if the {\tt malloc} call were in thread 1 instead of thread 2, the bug would still be nondeterministic, requiring MC to expose.

\begin{definition}[Use after free]
	Any read or write to heap memory which was once allocated, but no longer is.
\end{definition}

These can immediately be identified as failures by a MC which tracks allocation state.
%Most commonly this refers to accesses to a region already freed, but for brevity we also include

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Intuition}

This section provides (hopefully) intuitive summaries of our proof goals.
%for readers not interested in double-checking the proofs' internal structure.

{\bf Intuition for Iterative Deepening convergence.}
%In summary, we are proving
We will prove
that when Iterative Deepening saturates the set of data-race PPs,
that set represents every instruction where a preemption could possibly affect the program's behaviour
Hence, completing the associated state spaces is as strong a verification as testing all possible thread interleavings by preempting anywhere.
A data-race may be hidden in control-flow paths reachable only after preempting on a different data-race,
but the technique's iterative nature will eventually find it.
On the other hand, relying on the soundness of DPOR, preempting on an instruction which is neither a data-race or sync API boundary cannot affect the program's behaviour,
so any such PPs are irrelevant.

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& \multicolumn{2}{c}{\texttt{struct x \{ int foo; int baz; \} *x;}} \\
	& \multicolumn{2}{c}{\texttt{struct y \{ int bar; \} *y;~~~~~~~~~~}} \\
	\\
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	2 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	3 & & \texttt{\hilight{commentblue}{// x's memory recycled}} \\
	4 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	5 & & \texttt{\hilight{commentblue}{// ...initialize...}}\\
	6 & & \texttt{publish(y);} \\
	7 & & \texttt{\hilight{brickred}{y->bar = ...;}} \\
\end{tabular}
\caption{False-positive malloc-recycle pattern. This is the common case for which we avoid creating new state spaces.}
\label{fig:recycle}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & \texttt{publish(x1);} & \\
	2 & & \texttt{x2 = get\_published\_x();} \\
	3 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	4 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	5 & & \texttt{\hilight{commentblue}{// x's memory recycled}} \\
	6 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	7 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{Adversarial program which fits the malloc-recycle pattern, but nevertheless contains a true race.}
\label{fig:recycle-bug}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & \texttt{publish(x1);} & \\
	2 & & \texttt{x2 = get\_published\_x();} \\
	3 & & \texttt{\hilight{commentblue}{// x not free, so malloc's}} \\
	4 & & \texttt{\hilight{commentblue}{// return value changes!}} \\
	5 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	6 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	7 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	8 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
\caption{Goal interleaving, reordering the adversarial threads away from the pattern, while the data race remains.}
\label{fig:recycle-goal}
\end{figure}

Note that while we defined synchronization API PPs to occur both before and after any $\mathcal{A}_{sync}$ instruction,
we will only add data-race PPs {\em before} their associated $\mathsf{read}$ or $\mathsf{write}$.
Our proof requires preempting only before each racing instruction, not after,
in order to fully saturate the PP set with all possible data races.
Then, once saturated, preempting after each racing instruction would be extraneous,
because all subsequent instructions before the next PP must be local operations or non-communicating reads/writes%.
% Adding a PP both before and after could potentially expose some bugs sooner,
% if the 'after' PP were redundant with another data-race PP but that other race would take forever to find.
% So, I think it's an optimization to do it this way (although I haven't done a comparison experiment).
%
\footnote{Our implementation also optimizes the synchronization API PPs,
generally preempting only {\em after} each synchronization instruction.
$\mathsf{mutex\_lock}$ is the exception, as it can cause other threads to become blocked.
All the others can only make blocked threads runnable, establishing MHB,
which also provides MHB for the preceding transition of the invoking thread.
Hence, coalescing those transitions does not exclude any possible interleavings.
% If a third thread could "query" whether the 2nd thread is still blocked (or nonexistent),
% that would break the soundness of this optimization, and you'd need to preempt before mx_unlock and thr_create after all.
% XXX: In the implementation, such queries are actually possible with m_r or yield;
% while they do not interact with mutex_unlock, I think an adversary could use them to interleave with thr_create/exit.
}.

{\bf Intuition for malloc-recycle soundness.}
%In summary, we are proving
We prove that if a malloc-recycle-pattern data race is not a false positive, %a true race, rather than a false positive,
then DPOR %is guaranteed to ``reorder away the free and re-malloc''.
%In other words, DPOR's exploration
will eventually interleave threads in such a way that the malloc-recycle pattern will disappear,
while the access pair remains for the data-race detector to find, as shown in Figure~\ref{fig:recycle-goal}.
Hence, in the same state space where the malloc-recycle data race was found, if it's a true race, the same race will also appear without the recycle pattern.
So if that race can lead to a failure, Iterative Deepening will still be led to the necessary preemption point to find it.
