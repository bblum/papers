\section{Proof of malloc-recycle soundness}
\label{sec:proof}

We seek to prove that ignoring malloc-recycle data race candidates cannot cause DPOR + Iterative Deepening to miss a bug
that could be found by using the race as a PP predicate. Our soundness statement is as follows:

\begin{theorem}[Soundness of eliminating malloc-recycle races]
	If a malloc-recycle data race is not a false positive, DPOR will reorder threads such that either the same accesses will still race without fitting the malloc-recycle pattern, or a use-after-free bug will be reported immediately.
\end{theorem}

Though Figures~\ref{fig:recycle-bug} and \ref{fig:recycle-goal} show example programs, they do not capture all possible cases of how a true data race can fit the malloc-recycle pattern.
We proceed by establishing what must be true of any such program, then casing on the ambiguous possibilities, and showing that PPs will exist where we need them to reorder the threads.

For certain, there must be an access in one thread, followed by a free and malloc (we'll call them ``middle free'' and ``middle malloc''), each possibly from either thread, followed by an access from the other.
If the data race is not a false positive, then the second access must not change locations based on the middle malloc's return value.
WLOG, we say that thread 1 (T1) does the first access, called $a_1$, and thread 2 (T2) does the second, $a_2$.

\begin{lemma}
	If DPOR will reorder $a_2$ to before $a_1$, and the location of access $a_2$ doesn't change,
	then a non-malloc-recycle data race or a use-after-free bug will be identified.
	\label{lem:reorder}
\end{lemma}
\begin{proof}
By case on which threads the middle free and middle malloc came from.
\begin{itemize}
	\item T1 free, T2 malloc (as shown in Figure~\ref{fig:recycle-bug}). The malloc will go with $a_2$ to before the free, and because the allocation of concern has not been freed yet, will return a different value. Hence $a_1$ and $a_2$ will be in the same allocation; hence the race is not malloc-recycle anymore.
	\item T1 free, T1 malloc. Same as above, but the malloc does not move. The middle malloc will still recycle the memory, but $a_2$ now occurs before then, being in the same, older, allocation.
	\item T2 free, T2 malloc. Both the free and re-malloc will occur before either $a_1$ or $a_2$. The memory will be recycled and both accesses will appear to be in the later allocation.
	\item T2 free, T1 malloc. The free gets reordered earlier, the malloc stays put, and the accesses go in between. This will be a use-after-free bug.
\end{itemize}
If either the middle free or middle malloc came from a third thread, the case is the same as if it belonged to T1.
\end{proof}

The keen reader might ask here, what if T1 contains some extra spurious malloc calls (not related to $a_1$) that affect what T2's malloc returns after being reordered?
These could at best either cause {\tt x}'s memory to be recycled differently (not affecting the proof), or not at all (which simply causes immediate use-after-free).
%If such a malloc could affect $a_2$, the data race would be a false positive after all.
In general, extra spurious mallocs that could affect $a_1$ or $a_2$ could only convert the program back into a false-positive scenario;
and extra spurious synchronization events could only make it more easy to find PPs we need to trigger the reordering.
So we can safely assume the only relevant events are the ones we mention explicitly.

By our last assumption, there must also be an ``original malloc'' which allocated the block to begin with.
We must ask, which thread did the malloc which returned $a_1$'s address in the first place?
Our last assumption provides that the other thread must obtain that address through some communication mechanism (which we'll reason about later).

\subsection{T2 originally malloced x}

\begin{lemma}[Greedo]
	If T2 originally malloced the block containing {\tt x}, DPOR will reorder the threads to uncover a non-malloc-recycle race or a use-after-free bug.
	\label{lem:greedo}
\end{lemma}
\begin{proof}
Because T1 had the first access, there was a thread switch between the original malloc and $a_1$, as well as between $a_1$ and $a_2$. By Definition~\ref{def:transition}, each switch will be a PP.
By Definition~\ref{def:dpor}, DPOR will reorder $a_2$ to before $a_1$,
and because T1 is not involved in the logic determining $a_2$, the access's location stays the same.
Lemma~\ref{lem:reorder} finishes.
\end{proof}

This lemma also applies if a third thread was responsible for this malloc, as there would still be a thread switch in the same spot.

\subsection{T1 originally malloced x}

\begin{lemma}[Han]
	If T1 originally malloced the block containing {\tt x}, DPOR will reorder the threads to uncover a non-malloc-recycle race or a use-after-free bug.
	\label{lem:han}
\end{lemma}

\begin{proof}
We must guarantee there will be a PP during T1 before its $a_1$ access, but after whatever action it took to communicate the heap address to T2\footnote{
Note why we assert the publish action must come before $a_1$: otherwise, T2 couldn't be reordered to race $a_2$ with $a_1$ before T1 communicated the address, and it would be a false positive after all.}.

%\begin{figure}[t]
%	\small
%\begin{tabular}{rll}
%	& Thread 1 & Thread 2 \\
%	0 & \texttt{lock();} & \\
%	1 & \texttt{publish(x1);} & \\
%	3 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
%	4 & \texttt{\hilight{olivegreen}{free}(x1);} \\
%	0 & \texttt{unlock();} & \\
%	0 & & \texttt{lock();} \\
%	2 & & \texttt{x2 = get\_published\_x();} \\
%	0 & & \texttt{unlock();} \\
%	5 & & \texttt{// x's memory recycled} \\
%	6 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
%	7 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
%\end{tabular}
%\caption{If the locksets are the same in T1 during publish and x1, you can't avoid the data race on publish+get.}
%\label{fig:recycle-bug}
%\end{figure}

If there was a synchronization event between the publish action and $a_1$,
then the maximal state space assumption provides the necessary PP, and we are done.
%We ask: was there a synchronization event (unlock or message-pass) between the publish action and $a_1$?
%If so, then by the maximal state space assumption, the event will be a PP, and we are done.
%If not,
Otherwise, T1's lockset will be the same during publish and during $a_1$ (and the MHB-ness cannot change).
For T1's publish to reach T2, they must access the same memory ({\em outside} of the block containing $a_1$; T2 doesn't have that yet),
which we'll call $p$.
Hence, $p$ must be a data race of its own.
%(whether by writing {\tt x}'s address directly into the shared memory, or by sharing a pointer to a data structure containing {\tt x} -- doesn't matter which)

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\

	%1 & \texttt{p1 = \hilight{olivegreen}{malloc}(...);} & \\
	1 & \texttt{\hilight{brickred}{p1->ptr = x1;}} & \\
	2 & \texttt{publish(p1);} & \\
	3 & \texttt{\hilight{olivegreen}{free}(p1);} & \\
	4 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	5 & \texttt{\hilight{olivegreen}{free}(x1);} \\


	6 & & \texttt{p2 = get\_published\_p();} \\
	7 & & \texttt{\hilight{commentblue}{// p's memory recycled}} \\
	8 & & \texttt{q = \hilight{olivegreen}{malloc}(sizeof *q);} \\
	9 & & \texttt{\hilight{brickred}{x2 = p2->ptr;}} \\


	10 & & \texttt{\hilight{commentblue}{// x's memory recycled}} \\
	11 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	12 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{If the accesses used to publish {\tt x}'s address are a data race, their PPs may also be eliminated under the malloc-recycle pattern. Induction on the pointer structure leading to {\tt x} handles this case.}
\label{fig:induction}
\end{figure}

Because $p$ may also be a malloc-recycle data race,
as shown in Figure~\ref{fig:induction},
we do not necessarily get the PP for free.
In this case we need to prove that DPOR will likewise reorder any intermediate malloc-recycle pattern to generate the PP we need\footnote{
In \quicksand, data race PPs are not used immediately, but rather generate new state spaces to explore in the future. Anyway, under the maximal state space assumption, we will get to it eventually.}.
We handle this with induction on T2's pointer chain leading to {\tt x}.

\newcommand\publish[1]{$p_{#1}$}
\begin{itemize}
	\item For the base case, the publish location \publish{0} is either in global memory, or shared directly using synchronization.
		Non-heap memory data races are not subject to the malloc-recycle pattern, so will always get a data-race PP,
		and use of synchronization always gets a PP in the maximal state space.
	\item For the inductive step, a pointer \publish{n} is published in some heap memory \publish{n-1}\texttt{->ptr},
		and we assume that however \publish{n-1} is shared to T2,
		there will be a PP there
		% where \publish{n-1} is shared
		sufficient to make the \publish{n-1}\texttt{->ptr} access not malloc-recycle after DPOR.
		%DPOR will test some interleaving in which T1's
		Hence a data race PP will be generated on the \publish{n-1}\texttt{->ptr} access,
		and by Definition~\ref{def:dpor} and Lemma~\ref{lem:reorder},
		DPOR will reorder T1's and T2's subsequent accesses to \publish{n} sufficiently to place a PP on them.
		% i.e., so that the p_n access no longer appears to be malloc-recycle.
\end{itemize}

Hence, even if the accesses by which T1 shares {\tt x} with T2 appear in a different malloc-recycle pattern,
a PP will be identified on the publish location $p$,
%Hence by Definition~\ref{def:dpor}
and DPOR will reorder T2's execution to just after the publish action.
% XXX: Landslide does not do this! It only puts a PP immediately *before* the access. That's incompatible with this, which needs you to put one both before and after.
As long as T2's execution occurs after the publish, it will receive the same value for its $a_2$, so the location of the data race does not change.
Lemma~\ref{lem:reorder} concludes.
%Hence for any type of publish action, there will be a PP between it and $a_1$, hence DPOR will reorder $a_2$ to before $a_1$ (without changing the address at which $a_2$ occurs), and Lemma~\ref{lem:reorder} finishes.
\end{proof}

%We will also say that T1 does the initial malloc which first returns $a_1$'s address; although certainly T2 or even a third thread could have malloced it, that would involve a thread switch to thread 1, and hence a PP by Definition~\ref{def:transition}.
%As we will see, the main challenge of the proof is showing that enough PPs will exist at appropriate points, so adding more PPs by considering other threads for the initial malloc can only make the proof easier.

\subsection{Conclusion}

\setcounter{theorem}{2}
\begin{theorem}[Soundness of eliminating malloc-recycle races]
	\label{thm:recycle}
	If a malloc-recycle data race is not a false positive, DPOR will reorder threads such that either the same accesses will still race without fitting the malloc-recycle pattern, or a use-after-free bug will be reported immediately.
\end{theorem}
\begin{proof}
	Between Lemmas \ref{lem:greedo} and \ref{lem:han}, all cases of possible program structure are covered.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Heap overruns}
\label{sec:owned}

If we relax the ``sharing heap addresses'' assumption, there is another way to share the allocation's address without T1 and T2 communicating outside of $a_1$/$a_2$.
One thread can overrun a {\em different} heap block adjacent to the one containing $a_1$/$a_2$ (call them the ``neighbour block'' and ``real block'' respectively).
Figure~\ref{fig:overrun} shows an example.
Heap overrun bugs are quite serious \cite{eternal-war}, so we do not wish to exclude them from our proof.

In our evaluation, we used the full ``sharing heap addresses'' assumption to heuristically reduce state space size,
skipping reorderings which could only change the addresses allocated by $\mathsf{malloc}$.
This restricted our tests' scope to exclude such heap-overflow bugs.
We consider this justified because recent techniques \cite{sparc-ssm} can find such bugs quickly without the need for data-race PPs.
However, for users wishing to test for this class of bug and concurrency bugs simultaneously,
we show now how to strengthen the configuration of DPOR to cover the weakened assumption.

Note also that even if malloc-recycle candidates are {\em not} suppressed,
a DPOR which ignores malloc's internal metadata accesses would still be unsound with respect to these bugs.
We illustrate this in Figure~\ref{fig:overrun-notenough}:
even with PPs in arbitrary places, the two threads' transitions conflict only on $\mathsf{malloc}$'s internal metadata,
and hence DPOR would not attempt to reorder them.
Hence, our proofs so far show that suppressing malloc-recycle candidates is sound with respect to the classes of bugs which DPOR is already sound to.

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & & \texttt{z = malloc(42);} \\
	2 & & \texttt{\hilight{commentblue}{// TODO bounds check??}} \\
	3 & & \texttt{x2 = \&z[50];} \\
	4 & \texttt{x1 = malloc(...);} & \\
	5 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	6 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	7 & & \texttt{\hilight{commentblue}{// x's memory recycled}} \\
	8 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	9 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{Final possibility for how T2 can share T1's allocation address, and probably a security vulnerability to boot!}
\label{fig:overrun}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & & \texttt{z = malloc(42);} \\
	%2 & & \texttt{\hilight{commentblue}{// TODO bounds check??}} \\
	2 & & \texttt{x2 = \&z[50];} \\
	3 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	4 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	5 & \texttt{x1 = malloc(...);} & \\
	6 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	7 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
\caption{Without a PP between lines 4 and 5 of Figure~\ref{fig:overrun}, this is the only alternate interleaving DPOR would explore. The mallocs have been reordered and may no longer collide, which wrongly appears to be a false positive.}
\label{fig:overrun-notenough}
\end{figure}


\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& {\bf Thread 1} & {\bf Thread 2} \\
	1 & & \texttt{z = malloc(42);} \\
	%2 & & \texttt{\hilight{commentblue}{// TODO bounds check??}} \\
	2 & & \texttt{x2 = \&z[50];} \\
	3 & \texttt{x1 = malloc(...);} & \\
	4 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	5 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	6 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	7 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
	\caption{Goal interleaving of Figure~\ref{fig:overrun}. To ensure collision, the sequence of malloc calls producing $a_1$ and $a_2$ must not be disrupted compared to the original interleaving.}
	%, compared to the trace which originally exposed the malloc-recycle race.}
\label{fig:overrun-goal}
\end{figure}

% For shown it is in complexity class, not possible to complete,
To soundly suppress heap-overrun malloc-recycle candidates,
%we must consider the cases where T1 mallocs the real block and T2 overflows a neighbour, and vice versa.
we must strengthen our configuration of DPOR as follows:
accesses from $\mathsf{malloc}$'s internal implementation are not ignored when computing shared memory conflicts,
and the synchronization API PP predicates are extended to include the start and end of each $\mathsf{malloc}$ and $\mathsf{free}$ call.
%\footnote{
%	As a practical matter, in a large codebase where parts of it are trusted to avoid heap overflows, while others are not,
%	a user could use {\sc Landslide}'s {\tt within\_function} command to restrict such PPs, as we describe in the main paper.
%}.

\begin{lemma}
	If T1 and T2 each malloced neighbouring blocks, and collided based on pointer arithmetic involving no shared memory accesses,
	%did not communicate via any publish action $p$,
	%and the DPOR configuration is strengthened as above,
	DPOR will reorder the threads to uncover a non-malloc-recycle data race.
	\label{lem:leia} % "there is another"
\end{lemma}

\begin{proof}
WLOG, let T1's access in the original malloc-recycle race occur first.
We require that our strengthened DPOR will reorder T2's racing access to before T1's, such that both still occur on the same address.
There will be a PP in T1's execution between its $\mathsf{malloc}$ call and the subsequent racing access.
If there are no other memory conflicts between T1 and T2, then by the soundness of DPOR, this PP suffices to reorder without changing the address.
Otherwise, let $p$ be the latest conflicting access by T1 before its access $a_1$.
By the same inductive reasoning as we used in Lemma~\ref{lem:han}, Iterative Deepening will add a data-race PP on $p$.
As T1 has no further conflicts between $p$ and $a_1$, T2's $a_2$ will be reordered between them without changing the address.
Lemma~\ref{lem:reorder} finishes.
\end{proof}

Theorem~\ref{thm:recycle} is modified to simply include this lemma in addition to Lemmas~\ref{lem:greedo} and \ref{lem:han}.

%This spells QED so we are done.
