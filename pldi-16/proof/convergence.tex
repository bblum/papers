\section{Proof of Iterative Deepening convergence}

We seek to prove that, given enough time, Iterative Deepening offers the same coverage of thread interleavings that could be achieved by preempting at every single instruction.
We'll call the latter the {\em na\"{i}ve state space}, and call the condition of testing all its interleavings {\em convergence}.
(It could also be called soundness, from the perspective of viewing Iterative Deepening as a search re-ordering heuristic that doesn't miss any interleavings.)
Hence, our convergence statement is as follows:

\begin{theorem}[Total verification]
	If Iterative Deepening fully saturates its set of data-race PPs and completes all associated state spaces,
	it serves as a verification of all possible thread schedules of the given test program.
	\label{thm:totalverif}
\end{theorem}

The contrapositive statement offers more structure for an easier proof\footnote{
For the reader who likes to avoid non-constructive proofs, % \cite{vargomax}, % TODO: uncomment for camready/TR
note that we are using the constructive contrapositive direction:
Theorem~\ref{thm:convergence} is $A \rightarrow B$ and Theorem~\ref{thm:totalverif} is $\neg B \rightarrow \neg A$.}:

%DPOR + Iterative Deepening will eventually test every nonequivalent thread interleaving that would be exposed by preempting at every single instruction
\begin{theorem}[Convergence of Iterative Deepening]
	If a bug is exposed by an interleaving in the na\"{i}ve state space, Iterative Deepening will eventually test an equivalent interleaving which exposes the same bug.
	\label{thm:convergence}
\end{theorem}

\newcommand\ppnext[1]{\ensuremath{\mathsf{next}(#1)}}
\newcommand\ppinstr[1]{\ensuremath{\mathsf{instr}(#1)}}
\newcommand\ppothers[1]{\ensuremath{\mathsf{others}(#1)}}
\newcommand\pppfx[1]{\ensuremath{\mathsf{pfx}(#1)}}
\newcommand\conflicts[2]{\ensuremath{\mathsf{conflicts}(#1,#2)}}
\newcommand\pai{\ensuremath{p_{\alpha{}i}}}
\newcommand\tai{\ensuremath{t_{\alpha{}i}}}
Let
\[
	\mathcal{I} = \{(t_{\alpha{}1}, p_{\alpha{}1}), (t_{\beta{}1}, p_{\beta{}1}), ... (t_{\alpha{}n}, p_{\alpha{}n}), ...\}
\]
be an interleaving trace, where the element $(\tai, \pai)$ represents the $i$th transition $t$ of thread $\alpha$, and the associated preemption point $p$.
We will use the following notation:
\begin{itemize}
	\item $\ppnext{\pai}$ indicates the next transition by the same thread, $t_{\alpha{}(i+1)}$,
	\item $\ppinstr{\pai}$ indicates the first instruction executed during $t_{\alpha{}(i+1)}$. and
	\item $\ppothers{\pai}$ indicates the set of all transitions by other threads between $\tai$ and $t_{\alpha{}(i+1)}$.
	\item $\conflicts{t_\alpha,t_\beta}$ indicates the set of memory access pairs between $t_\alpha$ and $t_\beta$ such that among each pair, one access belongs to each thread $\alpha$ and $\beta$, the address is the same, and at least one is a write; i.e., precisely the shared memory conflict relation required by DPOR to determine dependent interleavings.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Equivalence of irrelevant PPs}

Iterative Deepening will not, of course, preempt on exactly the same instructions that an arbitrary naive interleaving would,
as it is only capable of preempting on data races and sync API boundaries.
Thus, our first task is to show that all naive interleavings have an equivalent interleaving with only data-race and sync API PPs.

\begin{definition}[Relevant preemption point]
	We say a PP $\pai$ is {\em relevant} if either $\ppnext{\pai}$ is a sync API boundary (including a voluntary yield),
	or if
	\[
		\conflicts{\ppothers{\pai}}{\ppinstr{\pai}} \ne \emptyset
	\]
	i.e., the instruction by thread $\alpha$ immediately after $\pai$ has a memory conflict with some other thread interleaved during $\pai$.
\end{definition}
\begin{definition}[Fully-relevant interleaving]
	An interleaving comprised only of relevant PPs.
\end{definition}

\begin{lemma}
	For any interleaving in the na\"{i}ve state space, there exists an equivalent interleaving which uses only relevant PPs.
	%data-race PPs and sync API PPs.
	\label{lem:equivalent}
\end{lemma}

\begin{proof}
	Let $\pai$ be the first irrelevant PP of a naive interleaving $\mathcal{I}$.
	We ask, did some thread among $\ppothers{\pai}$ conflict with any instruction from $\ppnext{\pai}$, even though there was no conflict with $\ppinstr{\pai}$?
	\begin{itemize}
			% rho for relevant.
		\item If $\conflicts{\ppothers{\pai}}{\ppnext{\pai}} \ne \emptyset$, or if $\tai$ contains a sync API boundary instruction that's not already a PP,
			then let $\rho$ be either the first instruction by $\ppnext{\pai}$ among the conflicts or the first sync API instruction, whichever comes first.
			Let $\pppfx{\rho}$ denote the instruction sequence between $\ppinstr{\pai}$ and $\rho$, including the former but not the latter.
			Now, we output the new interleaving:

			\begin{tabular}{lll}
				\\
				$\mathcal{I}' = \{...,$&$(\tai \cup \pppfx{\rho}, \pai'),$& \\
									   &$\ppothers{\pai},$& \\
					       &$(t_{\alpha{}(i+1)}) \setminus \pppfx{\rho}, p_{\alpha{}(i+1)}),$ & $ ...\}$ \\
				\\
			\end{tabular}

			In other words, we have simply reordered $\ppothers{\pai}$ to between $\pppfx{\rho}$ and $\rho$, removing the irrelevant $\pai$ and adding a new PP $\pai'$, which is relevant by the construction of $\rho$.
			We know it must be possible to reorder $\ppothers{\pai}$ to after $\alpha$'s execution because no synchronization is done during $\pppfx{\rho}$, hence the other threads' runnability cannot be affected.
			Likewise $\pppfx{\rho}$ is independent with $\ppothers{\pai}$, so by the soundness of DPOR, $\mathcal{I}' \equiv \mathcal{I}$.
		\item Otherwise, $\conflicts{\ppothers{\pai}}{\ppnext{\pai}} = \emptyset$, and no instructions among $\ppnext{\pai}$ are a sync API boundary.
			Then we output the new interleaving:
			\[
				\mathcal{I}' = \{..., (\tai \cup t_{\alpha(i+1)}, p_{\alpha(i+1)}), \ppothers{\pai},...\}
			\]
			In other words, we have reordered {\ppothers{\pai}} with the entire next transition by thread $\alpha$.
			We know this must be possible for the same reasons as above, and again, the transitions are independent, so $\mathcal{I}' \equiv \mathcal{I}$.
					       %&$(t_{\alpha{}(i+1)}) \setminus \pppfx{\rho}, p_{\alpha{}(i+1)}),$ & $ ...\}$ \\
	\end{itemize}
	This constitutes an algorithm for inductively converting (or removing) all irrelevant PPs to relevant ones.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Saturation of data-race PPs}

Now we must show that, starting from a sync-PP-only state space,
Iterative Deepening will eventually interleave threads sufficiently to expose all data races which are used as PPs in any buggy fully-relevant interleaving.
The challenge of this proof is that some data-race candidates are nondeterministic to find;
i.e., they may be hidden in an obscure control flow path which requires a prior preemption on a different data race to expose,
as shown in Figure~\ref{fig:nondet-dr}.
Hence, the maximal state space of statically-available sync API PPs will not necessarily uncover all possible data-race PPs;
Iterative Deepening may need to iterate through some data-race state spaces before finding certain nondeterministic races.

\begin{figure}[t]
	\small
	\begin{tabular}{rl}
	& \multicolumn{1}{c}{~\texttt{int x = 0, y = 0;~~~~~~~~~~~~~~~~}} \\
	& \multicolumn{1}{c}{\texttt{bool t1\_x = false, t1\_y = false;}} \\
	& \multicolumn{1}{c}{\texttt{bool t2\_x = false, t2\_y = false;}} \\
\\
		& Thread 1 (Thread 2 similar, with {\tt t1} and {\tt t2} vars swapped) \\
	1 & \texttt{\hilight{brickred}{x = x + 1;}} \\
	2 & \texttt{t1\_x = true;} \\
	3 & \texttt{\hilight{commentblue}{// "if x raced"}} \\
	4 & \texttt{if (x == 1 \&\& t2\_x) \{} \\
	5 & \texttt{~~~~\hilight{brickred}{y = y + 1;}} \\
	6 & \texttt{~~~~t1\_y = true;} \\
	7 & \texttt{~~~~\hilight{commentblue}{// "if y raced"}} \\
	8 & \texttt{~~~~if (y == 1 \&\& t2\_y) \{} \\
	9 & \texttt{~~~~~~~~panic();} \\
	10 & \texttt{~~~~\}} \\
	11 & \texttt{\}} \\
	\end{tabular}
	\caption{Not all data races will immediately be uncovered by interleaving threads around sync API PPs alone. Here, preempting during the race on line 1 is necessary to force the program into the control flow which can race on line 5.}
	\label{fig:nondet-dr}
\end{figure}

\begin{definition}[Reachable data race]
A data race candidate which will be identified by a MC configured to preempt only on locking API boundaries,
or transitively also configured to use data-race PPs of other
%already
reachable data races.
\end{definition}

\begin{definition}[Reachable preemption point]
A PP $\pai$ such that either $\ppinstr{\pai}$ is part of a reachable data race, or a sync API boundary.
\end{definition}

\begin{lemma}
	All PPs of any fully-relevant interleaving are reachable.
	\label{lem:saturation}
\end{lemma}

\newcommand\pbh{\ensuremath{p_{\beta{}h}}}
\newcommand\paj{\ensuremath{p_{\alpha{}j}}}
\newcommand\pbk{\ensuremath{p_{\beta{}k}}}
\newcommand\tbh{\ensuremath{t_{\beta{}h}}}
\newcommand\coalesce[1]{\ensuremath{\mathsf{coalesce}(#1)}}

\begin{proof}
Let $\mathcal{I} = \{(t_{\alpha{}1}, p_{\alpha{}1}), ...\}$ be the fully-relevant interleaving and PP sequence in the premise of the lemma which exposed a bug.
We proceed by induction on the PPs according to the order of their preemptions.
(Note that this is not necessarily the same as the order of the racing instructions $\ppinstr\pai$, which occur in $\ppnext\pai$, not in \tai.)
For both the base case and inductive step, we know (vacuously, for the former) that for each other PP $\pbh$ with $\tbh \prec \tai \in \mathcal{I}$, $\pbh$ is reachable.
We must show that a data race involving $\ppinstr\pai$
is reachable, and we are not allowed to use $\pai$ until we find the data race between $\ppinstr\pai$ and $\ppothers\pai$.

{\bf Coalescing not-yet-reachable data race PPs.}
Consider the alternate interleaving prefix:
\[
	\mathcal{J} = \{..., (\tai \cup \ppnext\pai', \paj), \ppothers\pai' \}
\]
where we have reordered $\ppnext\pai$ to before the execution of $\ppothers\pai$.
Here, $\paj$ is the first sync API PP in $\alpha$ after $\pai$ (as $p_{\alpha(i+1)}$ may be a not-yet-reachable data race PP),
and $\ppnext\pai'$ may include transitions of $\alpha$ beyond $\ppnext\pai$ itself.
%and for brevity, we will abbreviate the union of transitions as $\tak$.
Then, $\ppothers\pai'$ are the other threads' transitions from our target interleaving.
except they may be altered by the presence of some {\em other} data race in $\ppnext\pai'$ occurring after $\ppinstr\pai$.
Likewise, $\ppnext\pai'$ may be altered by some other data race in $\ppothers\pai'$.
The only certainty so far is that $\ppnext\pai'$ begins with $\ppinstr\pai$.

It would be straightforward to show that any other data races in $\ppnext\pai'$, which would be discovered in this interleaving,
could be reordered to after $\ppothers\pai'$,
eventually transforming $\ppnext\pai'$ to contain no conflicting memory accesses but $\ppinstr\pai$ itself.
Unfortunately, $\mathcal{J}$ is not necessarily reachable, as $\ppothers\pai'$ still includes any data-race PPs from $\ppothers\pai$,
which were ordered after $\pai$ in $\mathcal{I}$ and hence not covered by the inductive assumption.
Hence, we must perform the same ``coalescing'' for each thread in $\ppothers\pai'$ that we did for $\alpha$,
which we will abbreviate:
\[
	\mathcal{K} = \{..., (\tai \cup \ppnext\pai', \paj), \coalesce{\ppothers\pai'} \}
\]
For each other thread $\beta$, $\coalesce{\ppothers\pai'}$ will contain a single transition,
starting with the first instruction of $\beta$'s corresponding transition in $\ppothers\pai$,
and ending with the next sync API PP in $\beta$, which we'll call $\pbk$.
Again, the only certainly-executed instruction by $\alpha$ after $\tai$ is $\ppinstr\pai$.
It is even possible that neither $\ppinstr\pai$ nor any other instruction by thread $\alpha$ will conflict with any other thread,
until a different data race PP between two other threads is discovered,
as shown in Figure~\ref{fig:threethreads}.

It follows from the inductive hypothesis (which covers the prefix indicated by ``$...$''),
and from sync API PPs being reachable by definition,
that $\mathcal{K}$ is reachable.

\begin{figure}[t]
	TODO % TODO
	\caption{In this example, even though thread 1's data-race PP must occur first in a buggy interleaving, it cannot be reached until after Iterative Deepening first reaches the later-occurring data-race PP between threads 2 and 3.}
	\label{fig:threethreads}
\end{figure}

\newcommand\tbk{\ensuremath{t_{\beta{}k}}}
\newcommand\tbka{\ensuremath{t_{\beta{}k1}}}
\newcommand\tbkb{\ensuremath{t_{\beta{}k2}}}
\newcommand\tgl{\ensuremath{t_{\gamma{}l}}}

{\bf Finding the next reachable data-race PP.}
Now, we show by induction that a data race on $\ppinstr\pai$ will be reachable.
We assume that a state space $\mathcal{S}$ containing $\mathcal{K}$,
plus $n$ unique data race PPs among $\ppnext\pai'$ and $\ppothers\pai$,
will be reachable.
(In the base case, the number of new data-race PPs is 0, and $\mathcal{K}$'s reachability is justified above.)
We must show that in this state space, if the $\ppinstr\pai$ data race is not reachable,
a new unique data-race PP will instead be reachable.

By the definition of fully-relevant interleaving, $\mathcal{I}$ guarantees that some other thread $\omega$ can execute a data-racing instruction with $\ppinstr\pai$.
%
By the soundness of DPOR, if a program behaviour is possible by interleaving threads at the boundaries of
the given transitions, that interleaving will be tested.
By contrapositive,
because $\omega$'s data-racing instruction was not tested in $\mathcal{S}$, one or more necessary sites of interleaving must be in the middle of some transition, rather than at its boundaries.

However, we do not get a {\em single} such transition for free.
We have arrived at the crux of the proof:
to show that there cannot be multiple data-race PPs which must both be enabled before either data race can be identified.
Such a circular dependency seems intuitively impossible, but to actually find the ``first reachable'' PP is not straightforward.
We require that in $\mathcal{S}$, there exists a single transition $\tbk$ that can alone be split into $\{\tbka,\tbkb\}$,
and some other communicating transition $\tgl$ which conflicts with $\tbkb$.
%i.e., $\{\tbka, \tgl', \tbkb'\}$ exposes new program behaviour.

We show this by contradiction\footnote{
Regrettably, we could not devise a constructive algorithm for this part of the proof.
We leave actually identifying the first-reachable data-race PP to future work,
and trust that merely showing one must exist will satisfy the demands of modern verification.}:
%
Assume that for all $\tbk \in \mathcal{S}$,
and all possible points $\pbk'$ at which to split it into $\{\tbka,\tbkb\}$,
and all other non-MHB transitions $\tgl$,
$\tgl$ has no shared memory conflicts with $\tbkb$.
%
Let $\mathcal{S}' = \mathcal{S} \cup \pbk'$, i.e., the state space obtained by adding any such $\pbk'$ to $\mathcal{S}$'s PP-set.
By the soundness of DPOR, because any such $\tgl$ is independent with $\tbkb$,
\[
	\{..., \tbka, \tbkb, \tgl\}
	\equiv
	\{..., \tbka, \tgl', \tbkb'\}
\]
Hence, $\mathcal{S} \equiv \mathcal{S}'$. % This jump to full-state-space-equivalence seems a bit big, but the middle steps are all just more equivalence, obviously.
Then, the above assumption also applies to $\mathcal{S}'$,
which shows that for any {\em pair} of transitions such as $\tbk$, adding two new PPs cannot expose new program behaviour.
Hence, inductively, no set of new PPs of any size would expose new program behaviour not already exposed in $\mathcal{S}$.
However, the instruction by $\omega$ which conflicts with $\ppinstr\pai$ was not observed in $\mathcal{S}$,
so we have our contradiction.

Hence, if $\mathcal{S}$ does not expose the $\ppinstr\pai$ data race directly,
there must exist some transitions $\tbk = \{\tbka,\tbkb\}$ and $\tgl$ such that $\tgl$ shared-memory-conflicts with $\tbkb$ and could be interleaved immediately before it.
By the maximal state space assumption, all sync API PPs are already enabled, so the locksets of $\beta$ and $\gamma$ cannot overlap and there is no MHB relation.
%%% more verbose version
%so the locks held by $\beta$ cannot change during $\tbk$,
%and if $\tgl$ can be reordered to the middle of $\tbk$, the locksets of $\beta$ and $\gamma$ cannot overlap,
%and likewise there is no sync-enforced MHB relation.
Hence the memory conflict between $\tgl$ and $\tbkb$ will be identified as a data-race.
Finally, because $\tbk$ was not previously split by a PP, the data-race was not already discovered.

{\bf Reaching $\mathbf{p_{\alpha{}i}}$.}
Hence either a data-race with $\ppinstr\pai$ will be identified,
or an infinite/nonterminating sequence of other unique data-race PPs will be identified,
but by the halting assumption, this would constitute an infinite loop bug, which is sufficient.
(Alternatively, for any program with a finitely-sized instruction listing, the number of unique instruction pairs is finite.)
Hence by the ``next reachable data-race'' induction, $\pai$ is reachable.
Hence by the ``$\pai$ is reachable'' induction, all PPs in $\mathcal{I}$ are reachable.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Conclusion}

\setcounter{theorem}{1}
\begin{theorem}[Convergence of Iterative Deepening]
	If a bug is exposed by an interleaving in the na\"{i}ve state space, Iterative Deepening will eventually test an interleaving which exposes the same bug.
\end{theorem}

\begin{proof}
By Lemma~\ref{lem:equivalent}, there must be some equivalent fully-relevant interleaving.
%the na\"{i}ve interleaving must be equivalent to some interleaving built of data-race PPs and sync API PPs.
By Lemma~\ref{lem:saturation}, Iterative Deepening will eventually discover and enable all necessary data-race PPs, with sync API PPs being enabled by the maximal state space assumption\footnote{
		In the paper we noted that when a bug is found, \quicksand~cancels all superset state spaces, anticipating that they would find the same bug.
		Because we are concerned with verifying the absence of bugs, we say this also suffices for convergence, but for brevity, we will leave this condition implicit.}.
Then DPOR will find the buggy interleaving within this state space.
\end{proof}
