% TODO: Abbreviate SMC, and fixed-PP approach.

\section{Evaluation}

Although \quicksand~presents Iterative Deepening and data-race PPs as interconnected techniques, they each could theoretically be employed alone in other model checkers.
For example, a single-state-space tool could use data-race candidates during immediately subsequent interleavings, essentially changing the state space on the fly.
Likewise, a message-passing-only tool could employ Iterative Deepening despite data races being absent from its concurrency model.
Hence, though many of our experiments compare \quicksand~to the state-of-the-art as a whole,
we also sought to evaluate each technique individually.
Our evaluation answers the following questions:
\begin{enumerate}
	\item Does Iterative Deepening find bugs faster than SSS-MC in subset state spaces, even without data-race PPs?
	\item Is combining data-race analysis with MC productive?
		\begin{enumerate}
			\item Do data-race PPs expose new bugs that couldn't be found with SSS-MC's fixed-PP-set approach?
				% Elaborate later:
				% Among those, how many were missed in a {\em completed} execution of the otherwise ``maximal'' state space?
			\item Do we avoid false positives compared to a single-execution data race analysis?
				% Explain later as:
				% How many data-race candidates were verified as benign
				% But to be fair, you have to count how many DRs are reported as "couldn't test these, check yourself" at the end.
				% Also Include:
				% How many false positives does the free-re-malloc technique suppress?
				% TODO: If you have time, re-run all of the dr-only bug tests, with DR_FALSE_NEG enabled, and see how much fewer bugs get found (how many bugs get pushed past the time limit?)
			% TODO: This one's optional. You can give up on it.
			\item Do we avoid false negatives compared to single-pass?
				%TODO: for this experiment, set EXPLORE_BACKWARDS=0
				% TODO: And disable false-neg malloc-free technique
				% TODO: And also disable the confirmed/suspected thing
				% (where mem.c waits for reorder observed before
				% messaging the latter half of the DR to QS)
		\end{enumerate}
\end{enumerate}
\subsection{Test Suite}
% TODO: Maybe say how many lines of code total? How many lines on average per P2/pintos? (careful with pintos; lots of basecode)
Our test suite consists of \numthrlibs~``P2'' student thread libraries, from CMU's 15-410 OS class,
%across the Spring and Fall 2014 and Spring 2015 semesters;
and \numpintoses~``Pintos'' student kernels, from Berkeley's CS162 and U. Chicago's CS230 OS classes.
%
The P2 thread library comprises \texttt{thr\_create()}, \texttt{thr\_exit()}, \texttt{thr\_join()}, mutexes, condvars, semaphores, and r/w locks;
all implemented from scratch in userspace with a UNIX-like system call interface \cite{kspec,thrlib}.
%
The Pintos kernel project
involves implementing priority scheduling, \texttt{sleep()}, and user-space process management (\texttt{wait()} and \texttt{exit()})
using provided bare-bones mutex, context-switch, and virtal memory implementations
\cite{pintos}.
% P2 SLOC stats: 1807 avg; 1723 median; range 1181-4114.
% All numbers, obtained with:
% cd p2s; for i in */*; do wc -l $i/user/libthread/*.{c,h,S} $i/user/libthread/*/*.{c,h,S} ; done | grep total
% 1181 1192 1221 1230 1238 1240 1243 1261 1275 1307 1310 1318 1325 1334 1336 1345 1366
% 1388 1388 1403 1415 1416 1430 1451 1478 1498 1527 1589 1618 1635 1638 1654 1675 1676
% 1716 1719 1720 1723 1723 1727 1737 1743 1744 1751 1769 1777 1782 1789 1789 1812 1918
% 1926 1946 1994 2022 2043 2066 2077 2088 2099 2131 2164 2172 2190 2215 2227 2277 2282
% 2384 2387 2483 2486 2503 2514 2551 2597 2610 2665 4114
Though not ``real world'' programs, both projects are quite large: % maybe "complex"?
the P2s average 1807 lines of C and x86 assembly (stddev 489.5),
% Pintos SLOC stats: TODO
and the Pintoses average {\bf 9999999} % TODO

\newcommand\mxtest{\texttt{mx\_test}}
\newcommand\tej{\texttt{thr\_exit\_join}}
\newcommand\bct{\texttt{broadcast}}
\newcommand\paraguay{\texttt{paraguay}}
\newcommand\paradise{\texttt{paradise\_lost}}
\newcommand\rwl{\texttt{rwl\_test}}
We tested P2s with 6 multithreaded programs:
% from the 410 test suite % XXX: I would like to say this but this is a lie; figure out what else i can say instead
% each tailored to exercise a different part of the P2 project
\mxtest, for locking algorithm correctness, \tej, a test of thread lifecycle, \bct~and \paraguay, for condvars, \paradise~for semaphores, and \rwl~for r/w locks.
For \mxtest~we enabled \landslide's mutex-testing option described in \sect{\ref{sec:landslide}}.
%, which changes the lock-set tracking to consider accesses within {\tt mutex\_lock} and {\tt mutex\_unlock} as data-race candidates.
%(Normally, they are considered protected by the very lock they implement: assuming the locks are already correct to enable productive data-race analysis on the rest of the codebase.)
\newcommand\prisema{\texttt{priority\_sema}}
\newcommand\waitsimple{\texttt{wait\_simple}}
% TODO: Add more test cases
We tested the Pintoses with 2 programs from the class test suite: \prisema, a test of the kernel scheduling algorithm, and \waitsimple, a test of process lifecycle. % TODO talk more about these.
In total, this test suite comprises {\large \bf 99999} % TODO
unique state spaces.

All tests were run on 12-core 3.2 GHz Xeon machines with 12GB of RAM.
\quicksand~tests were given 10 CPUs for 1 hour each.
To compare to SSS-MC, we ran a control experiment for each test, running \landslide~on a single state space with all PPs enabled in advance (and no data-race PPs).
% TODO figure out somewhere to mention what landslide's pps are: mx lock/unlock (aka sema up/down)
Though \landslide~does not implement parallel DPOR \cite{parallel-dpor}, we leveled the playing field by giving each control 1 CPU for 10 hours and dividing all associated times by 10 (simulating perfect parallelism).

\begin{table}[t]
	\begin{tabular}{l|l|l}
			& QS bugs & SSS-MC bugs \\
		\hline
		\mxtest & eg 1000 & eg 0 \\
		\bct & & \\
		etc... & & \\
		\hline
		Total & & \\
	\end{tabular}
	\caption{Comparison of all bugs found, broken down by test case, among all P2s (top 6) and Pintoses (bottom {\bf \large 9999999})}
	\label{tab:allbugs}
\end{table}

\begin{table}[t]
	\small
	\begin{tabular}{l|l|l||l|l}
	& QS bug & \begin{tabular}{c} SSS-MC \\ completed\end{tabular}
	& QS bug & \begin{tabular}{c}SSS-MC \\ timeout \end{tabular} \\
		\hline
		\mxtest & e.g. 5 & 10 & 0 & 0 \\
		\bct & & & & \\
		etc... & & & & \\
		\hline
		Total & & & & \\
	\end{tabular}
	\caption{Bugs requiring data-race PPs to expose, found by \quicksand~but missed by the single-state-space approach.}
	\label{tab:drbugs}
\end{table}

\subsection{Evaluating Iterative Deepening}

asdfadsfasdf


\subsection{Evaluating Data-Race PPs}

In this section we show that using data-race PPs with \landslide~is more effective than either SSS-MC or single-pass data-race detection alone.

{\bf Finding bugs missed by SSS-MC.}
Though state-of-the-art MCs preempt only on synchronization events, many serious concurrency bugs are caused by data races leading to corrupted shared state.
In Table~\ref{tab:drbugs} we count ...... % TODO

In Table~\ref{tab:allbugs} .... %TODO

{\bf Avoiding false positive data-race candidates.}
% Though we mechanically verify whether each data race candidate leads to a bug, each new PP can increase combinatorially..... obviously wish to avoid...


{\bf Avoiding false negative data-race bugs.}
Some pairs of unprotected shared-memory accesses may be hidden in some control flow path that requires a nondeterministic preemption to be executed.
In such cases, a single-pass data-race detector
%could not achieve the coverage necessary
would fail
to identify the access pair as a data-race candidate to begin with.
%
We counted how many such data-races, used as PPs, would lead to \landslide~finding new bugs,
thereby making them {\em false negatives} of the single-pass approach.
% TODO: Put a figure here giving an example of where e.g. a data race only shows up during the contention path of a mutex.
We classified each data-race candidate according to whether \landslide~had reported them during the first interleaving, before any backtracking or preempting: if so, they were {\em deterministic data races} (hence could be found by single-pass).
%
To ensure a fair comparison, we disabled \landslide's {\em false-positive}-avoidance techniques during this experiment.
For example, we reported free-re-malloc data races during the first interleaving as {\em deterministic}, as a single-pass analysis must, rather than waiting until future interleavings to confirm them (see \sect{\ref{sec:free-re-malloc}}).

% Figure out concretely what the data race tricks are that we do, so we can claim them as contributions in the paper. Then ACTUALLY EVALUATE THEM.
%         - Speculative DR PPs.
%                 Not a heuristic, rather how to make it work at all to begin with.
%                 (Cite MS thesis, claim on backwards explorating finding bugs faster)
%         - Free/re-malloc to eliminate some false positives. See #193.
%                 Measure how many false positives are eliminated.
%                 Check, ofc, to make ABSOLUTE SURE, that no bugs missed w/ this trick.
%                         If there are, it could be because of the implementation
%                         bug described in #193.
%         - Using tid/last_call filtering because whole stack traces are too expensive.
%                 Moderately optional, 1st priority since theoretically interesting:
%                 Turn on/off and measure how resulting DR bug #s change.
%         - Optional: Reprioritizing DRs based on "confirmed" / "suspected"
%                 Shouldn't be hard just make ID wrapper print "s" or "c"!
%                 Is it helpful for ID to put priorities on DR PPs?
%                         Test by inverting the priority and see if fewer buges are found.
%         // Super optional to talk about. Probably not worth the time.
%         // - "Too suspicious" (during init/destroy)
%         //      (Cite eraser, section 2.2)

\subsection{Discussion} % and future work?

% TODO: Mention that SSSMC control never FAB in mutex_test whatsoever, while DR PPs enabled FABs.

%We tested 87 pintos kernels and found races in 45-47 of them, among which 34-47 of those bugs required data-race preemption points to expose. That means we rock.

% TODO: Future work.
% Future work: Add a way to configure even smaller subsets (eg "only mutex_locks called from site X") for cases where mx_lock and mx_unlock alone are still too big. Count the number of kernels for which this was the case.

% Future work: Add parallel DPOR so you can fill your spare CPUs when there are fewer than the max number of jobs.
