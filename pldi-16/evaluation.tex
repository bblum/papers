% TODO: Abbreviate SMC, and fixed-PP approach.

\section{Evaluation}

% TODO: Don't forget to add to quicksand a feature to print time-when-job-started (ie, tell total time from test start to bug found in a DR state space == job-start time + job-elapsed time)
% TODO ******BEFORE****** you begin running experimence en masse!

Although \quicksand~presents Iterative Deepening and data-race PPs as interconnected techniques, they each could theoretically be employed alone in other model checkers.
For example, a single-state-space tool could use data-race candidates during immediately subsequent interleavings, essentially changing the state space on the fly.
Likewise, a message-passing-only tool could employ Iterative Deepening despite data races being absent from its concurrency model.
Hence, though many of our experiments compare \quicksand~to the state-of-the-art as a whole,
we also sought to evaluate each technique individually.
Our evaluation answers the following questions:
\begin{enumerate}
	\item Is combining data-race analysis with stateless model checking productive?
		\begin{enumerate}
			\item Do data-race PPs expose new bugs that couldn't be found with single-state-space model checking?
				% Elaborate later:
				% Among those, how many were missed in a {\em completed} execution of the otherwise ``maximal'' state space?
			\item Do we avoid false positives compared to single-pass?
				% Explain later as:
				% How many data-race candidates were verified as benign
				% Also Include:
				% How many false positives does the free-re-malloc technique suppress?
			% TODO: This one's optional. You can give up on it.
			\item Do we avoid false negatives compared to single-pass?
				% Explain later as: How many data-race bugs required backtracking to be identified as candidates in the first place
				%TODO: for this experiment, set EXPLORE_BACKWARDS=0
				% TODO: And disable false-neg malloc-free technique
				% TODO: And also disable the confirmed/suspected thing
				% (where mem.c waits for reorder observed before
				% messaging the latter half of the DR to QS)
		\end{enumerate}
	\item Is Iterative Deepening better than the single-state-space model?
		\begin{enumerate}
			\item Does Iterative Deepening find bugs
				% in subset state spaces
				faster than the single-state-space approach, even without data races?
			\item Does Iterative Deepening help test data-race PPs?
				% Are there some DRs where the PP alone finds the bug, while the superset SS cannot, and some vice versa?
				% What we want is a *variety*, which proves that iterative deepening is better than either individual strategy.
		\end{enumerate}
\end{enumerate}
\subsection{Test Suite}
% TODO: Maybe say how many lines of code total? How many lines on average per P2/pintos? (careful with pintos; lots of basecode)
Our test suite consists of \numthrlibs~``P2'' student thread libraries, from CMU's 15-410 OS class,
%across the Spring and Fall 2014 and Spring 2015 semesters;
and \numpintoses~``Pintos'' student kernels, from Berkeley's CS162 and U. Chicago's CS230 OS classes.
%
The P2 thread library comprises \texttt{thr\_create()}, \texttt{thr\_exit()}, \texttt{thr\_join()}, mutexes, condvars, semaphores, and r/w locks;
all implemented from scratch in userspace with a UNIX-like system call interface \cite{kspec,thrlib}.
%
The Pintos kernel project
involves implementing priority scheduling, \texttt{sleep()}, and user-space process management (\texttt{wait()} and \texttt{exit()})
using provided bare-bones mutex, context-switch, and virtal memory implementations
\cite{pintos}.
% P2 SLOC stats: 1807 avg; 1723 median; range 1181-4114.
% All numbers, obtained with:
% cd p2s; for i in */*; do wc -l $i/user/libthread/*.{c,h,S} $i/user/libthread/*/*.{c,h,S} ; done | grep total
% 1181 1192 1221 1230 1238 1240 1243 1261 1275 1307 1310 1318 1325 1334 1336 1345 1366
% 1388 1388 1403 1415 1416 1430 1451 1478 1498 1527 1589 1618 1635 1638 1654 1675 1676
% 1716 1719 1720 1723 1723 1727 1737 1743 1744 1751 1769 1777 1782 1789 1789 1812 1918
% 1926 1946 1994 2022 2043 2066 2077 2088 2099 2131 2164 2172 2190 2215 2227 2277 2282
% 2384 2387 2483 2486 2503 2514 2551 2597 2610 2665 4114
Though not ``real world'' programs, both projects are quite large: % maybe "complex"?
the P2s average 1807 lines of C and x86 assembly (stddev 489.5),
% Pintos SLOC stats: TODO
and the Pintoses average {\bf 9999999} % TODO



% TODO: List test programs that were run.

\subsection{Evaluating Data-Race PPs}

In this section we show that using data-race PPs with \landslide~is more effective than either SSS-MC or single-pass data-race detection alone.

{\bf Finding bugs missed by SSS-MC.}
% TODO describe

{\bf Avoiding false positive data-race candidates.}
% TODO describe

{\bf Avoiding false negative data-race bugs.}
Some pairs of unprotected shared-memory accesses may be hidden in some control flow path that requires a nondeterministic preemption to be executed.
In such cases, a single-pass data-race detector
%could not achieve the coverage necessary
would fail
to identify the access pair as a data-race candidate to begin with.
%
We counted how many such data-races, used as PPs, would lead to \landslide~finding new bugs,
thereby making them {\em false negatives} of the single-pass approach.
% TODO: Put a figure here giving an example of where e.g. a data race only shows up during the contention path of a mutex.
We classified each data-race candidate according to whether \landslide~had reported them during the first interleaving, before any backtracking or preempting: if so, they were {\em deterministic data races} (hence could be found by single-pass).
%
To ensure a fair comparison, we disabled \landslide's {\em false-positive}-avoidance techniques during this experiment.
For example, we reported free-re-malloc data races during the first interleaving as {\em deterministic}, as a single-pass analysis must, rather than waiting until future interleavings to confirm them (see Section \ref{sec:free-re-malloc}).

\subsection{Evaluating Iterative Deepening}


% Figure out concretely what the data race tricks are that we do, so we can claim them as contributions in the paper. Then ACTUALLY EVALUATE THEM.
%         - Speculative DR PPs.
%                 Not a heuristic, rather how to make it work at all to begin with.
%                 (Cite MS thesis, claim on backwards explorating finding bugs faster)
%         - Free/re-malloc to eliminate some false positives. See #193.
%                 Measure how many false positives are eliminated.
%                 Check, ofc, to make ABSOLUTE SURE, that no bugs missed w/ this trick.
%                         If there are, it could be because of the implementation
%                         bug described in #193.
%         - Using tid/last_call filtering because whole stack traces are too expensive.
%                 Moderately optional, 1st priority since theoretically interesting:
%                 Turn on/off and measure how resulting DR bug #s change.
%         - Optional: Reprioritizing DRs based on "confirmed" / "suspected"
%                 Shouldn't be hard just make ID wrapper print "s" or "c"!
%                 Is it helpful for ID to put priorities on DR PPs?
%                         Test by inverting the priority and see if fewer buges are found.
%         // Super optional to talk about. Probably not worth the time.
%         // - "Too suspicious" (during init/destroy)
%         //      (Cite eraser, section 2.2)

We tested 87 pintos kernels and found races in 45-47 of them, among which 34-47 of those bugs required data-race preemption points to expose. That means we rock.

% TODO: Future work.
% Future work: Add a way to configure even smaller subsets (eg "only mutex_locks called from site X") for cases where mx_lock and mx_unlock alone are still too big. Count the number of kernels for which this was the case.
