% TODO: Abbreviate SMC, and fixed-PP approach.

\section{Evaluation}

Although \quicksand~presents Iterative Deepening and data-race PPs as interconnected techniques, they each could theoretically be employed alone in other model checkers.
For example, a single-state-space tool could use data-race candidates during immediately subsequent interleavings, essentially changing the state space on the fly.
Likewise, a message-passing-only tool could employ Iterative Deepening despite data races being absent from its concurrency model.
Hence, though many of our experiments compare \quicksand~to the state-of-the-art as a whole,
we also sought to evaluate each technique individually.
Our evaluation answers the following questions:
\begin{enumerate}

	\item Does \quicksand~improve upon state-of-the-art MC?
		\begin{enumerate}
			\item Does Iterative Deepening find bugs faster
				%than SSS-MC
				in subset state spaces, even without data-race PPs?
			% Probably not... ICB is state of the art here.
			% In large tests, can Iterative Deepening provide partial verification by completing smaller state spaces
			\item Do data-race PPs expose new bugs that couldn't be found with SSS-MC's fixed-PP-set approach?
				% Elaborate later:
				% Among those, how many were missed in a {\em completed} execution of the otherwise ``maximal'' state space?
		\end{enumerate}
	\item Does MC improve the accuracy of data-race detection?
		\begin{enumerate}
			\item Do we avoid false positives compared to a single-execution data race analysis?
				% Explain later as:
				% How many data-race candidates were verified as benign
				% But to be fair, you have to count how many DRs are reported as "couldn't test these, check yourself" at the end.
				% Also Include:
				% How many false positives does the free-re-malloc technique suppress?
				% TODO: If you have time, re-run all of the dr-only bug tests, with DR_FALSE_NEG enabled, and see how much fewer bugs get found (how many bugs get pushed past the time limit?)
			% TODO: This one's optional. You can give up on it.
			\item Do we find data-race bugs that would be false-negatives during a single-execution analysis?%Do we avoid false negatives compared to single-pass?
				%TODO: for this experiment, set EXPLORE_BACKWARDS=0
				% TODO: And disable false-neg malloc-free technique
				% TODO: And also disable the confirmed/suspected thing
				% (where mem.c waits for reorder observed before
				% messaging the latter half of the DR to QS)
		\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Test Suite}
% TODO: Maybe say how many lines of code total? How many lines on average per P2/pintos? (careful with pintos; lots of basecode)
Our test suite consists of \numthrlibs~``P2'' student thread libraries, from CMU's 15-410 OS class,
%across the Spring and Fall 2014 and Spring 2015 semesters;
and \numpintoses~``Pintos'' student kernels, from Berkeley's CS162 and U. Chicago's CS230 OS classes.
%
The P2 thread library comprises \texttt{thr\_create()}, \texttt{thr\_exit()}, \texttt{thr\_join()}, mutexes, condvars, semaphores, and r/w locks;
all implemented from scratch in userspace with a UNIX-like system call interface \cite{kspec,thrlib}.
%
The Pintos kernel project
involves implementing priority scheduling, \texttt{sleep()}, and user-space process management (\texttt{wait()} and \texttt{exit()})
using provided bare-bones mutex, context-switch, and virtal memory implementations
\cite{pintos}.
% P2 SLOC stats: 1807 avg; 1723 median; range 1181-4114.
% All numbers, obtained with:
% cd p2s; for i in */*; do wc -l $i/user/libthread/*.{c,h,S} $i/user/libthread/*/*.{c,h,S} ; done | grep total
% 1181 1192 1221 1230 1238 1240 1243 1261 1275 1307 1310 1318 1325 1334 1336 1345 1366
% 1388 1388 1403 1415 1416 1430 1451 1478 1498 1527 1589 1618 1635 1638 1654 1675 1676
% 1716 1719 1720 1723 1723 1727 1737 1743 1744 1751 1769 1777 1782 1789 1789 1812 1918
% 1926 1946 1994 2022 2043 2066 2077 2088 2099 2131 2164 2172 2190 2215 2227 2277 2282
% 2384 2387 2483 2486 2503 2514 2551 2597 2610 2665 4114
Though not ``real world'' programs, both projects are quite large: % maybe "complex"?
the P2s average 1807 lines of C and x86 assembly (stddev 489.5),
% Pintos SLOC stats: TODO
and the Pintoses average {\bf 9999999} % TODO

\newcommand\mxtest{\texttt{mx\_test}}
\newcommand\tej{\texttt{thr\_exit\_join}}
\newcommand\bct{\texttt{broadcast}}
\newcommand\paraguay{\texttt{paraguay}}
\newcommand\paradise{\texttt{paradise\_lost}}
\newcommand\rwl{\texttt{rwl\_test}}
We tested P2s with 6 multithreaded programs:
% from the 410 test suite % XXX: I would like to say this but this is a lie; figure out what else i can say instead
% each tailored to exercise a different part of the P2 project
\mxtest, for locking algorithm correctness, \tej, a test of thread lifecycle, \bct~and \paraguay, for condvars, \paradise~for semaphores, and \rwl~for r/w locks.
For \mxtest, \paradise, and \paraguay, we used {\tt without\_function} to blacklist {\tt thr\_create}, {\tt thr\_exit}, and {\tt thr\_join},
and for \mxtest~we enabled \landslide's mutex-testing option
(see \sect{\ref{sec:landslide}}).
\newcommand\prisema{\texttt{priority\_sema}}
\newcommand\waitsimple{\texttt{wait\_simple}}
% TODO: Add more test cases
We tested the Pintoses with 2 programs from the class test suite: \prisema, a test of the kernel scheduling algorithm, and \waitsimple, a test of process lifecycle system calls.
For all tests, we used {\tt without\_function} to blacklist PPs on the {\tt malloc} mutex.
% XXX: Some pintoses can't run all the tests. So this number is too high.
In total, this test suite comprises 632 unique state spaces.
All tests were run on 12-core 3.2 GHz Xeon machines with 12GB of RAM.

\begin{table}[t]
	\begin{tabular}{l|l|l}
			& QS bugs & SSS-MC bugs \\
		\hline
		\mxtest & eg 1000 & eg 0 \\
		\bct & & \\
		etc... & & \\
		\hline
		Total & & \\
	\end{tabular}
	\caption{Comparison of all bugs found, broken down by test case, among all P2s (top 6) and Pintoses (bottom 2)}
	\label{tab:allbugs}
\end{table}

\begin{table}[t]
	\small
	\begin{tabular}{l|l|l||l|l}
	& QS bug & \begin{tabular}{c} SSS-MC \\ completed\end{tabular}
	& QS bug & \begin{tabular}{c}SSS-MC \\ timeout \end{tabular} \\
		\hline
		\mxtest & e.g. 5 & 10 & 0 & 0 \\
		\bct & & & & \\
		etc... & & & & \\
		\hline
		Total & & & & \\
	\end{tabular}
	\caption{Bugs requiring data-race PPs to expose, found by \quicksand~but missed by the single-state-space approach.}
	\label{tab:drbugs}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Comparing Iterative Deepening to SSS-MC}
\label{sec:eval-sssmc}

% TODO: If you have time, rerun all the quicksand experiments JUST running the 4 base state spaces. Give it 10/4 hours of cpu budget on 4 cores. John's expt.

% mention exactly which state spaces we are comparing here
% mention partial verification in terms of state space completion, when SSSMC times out.

%In this section we show that using data-race PPs with \landslide~is more effective than either SSS-MC or single-pass data-race detection alone.

To compare to SSS-MC, we ran a control experiment for each test, running \landslide~on a single state space with all PPs on sync primitives enabled in advance (and no data-race PPs).
We gave each \quicksand~test 10 CPUs for 1 hour each. % XXX
% TODO figure out somewhere to mention what landslide's pps are: mx lock/unlock (aka sema up/down)
Though \landslide~does not implement parallel DPOR \cite{parallel-dpor}, we compensated by giving each control test 1 CPU for 10 hours,
%then dividing all associated times by 10 (simulating perfect parallelism).
and instrumenting \quicksand~to report total CPU-hours rather than wall-clock time.
%\quicksand's times by 10 to convert from wall-clock time to CPU-hours (even though it sometimes falls short of 100\% parallelism).
Figure~\ref{fig:dowefindbugsfaster} plots the bug-finding speed of SSS-MC against that of two different \quicksand~experiments:
%which we explain presently.

\begin{figure}[t]
	\includegraphics[width=0.48\textwidth]{dowefindbugsfaster.pdf}
	\caption{Comparison of bug-finding performance
	by several configurations of \quicksand~and the SSS-MC control.
	\quicksand~finds 169\% as many bugs with data-race PPs.}
	\label{fig:dowefindbugsfaster}
\end{figure}

{\bf Finding the same bugs faster.}
To show that Iterative Deepening is effective even for MC domains without data races, such as message-passing distributed systems,
we ran the test suite with \quicksand~configured to explore only subsets of the hard-coded mutex PPs (i.e., ignoring all data-race reports)\footnote{
Because \quicksand~is not yet instrumented to subset hard-coded PPs beyond the 4 ways shown in Figure~\ref{fig:id},
we ran these tests for 2.5 hours on 4 CPUs each.
Future work could parallelize QS-no-DR-PPs further; see \sect{\ref{sec:future}}.}.
%
The line QS-no-DR-PPs represents this experiment;
% TODO: rephrase based on result of expt
we see that even though SSS-MC mostly catches up to it by the end of the 10-hour budget,
QS-no-DR-PPs finds many more of the bugs much sooner.
From this we conclude that for smaller arbitrary CPU budgets,
Iterative Deepening is likely to find bugs SSS-MC will miss,
and programmers can be more confident in the verification provided when \quicksand~times out with no bug found.

{\bf Finding new data-race bugs.}
Though state-of-the-art MCs preempt only on synchronization events, many serious concurrency bugs are caused by data races leading to corrupted shared state.
The line QS-DRs represents our ``no holds barred'' \quicksand~tests:
we quickly pull ahead of SSS-MC, and ultimately conclude with 69\% more bugs in total.
The break-even point is at a negligible 45 seconds.

Furthermore, we plotted another line from this dataset, QS-no-DR-bugs,
which represents only the bugs found in state spaces without data-race PPs (like QS-no-DR-PPs, but even when data-race PPs are enabled).
Intuitively, this line shows that for programs with only benign data races,
\quicksand~can afford the extra overhead of verifying them while still slightly edging out SSS-MC\footnote{
The initial perfect overlap between QS-DRs and QS-no-DR-bugs indicates how long it takes before the first data-race bug is found.}.
%even after the extra overhead of verifying them, \quicksand~still slightly edges out SSS-MC

{\bf Partial verification guarantees.}
TODO % TODO

%Hence, in Table~\ref{tab:drbugs} we count how often \quicksand~uncovered a bug only in state spaces which included data-race PPs, while
%
%In Table~\ref{tab:allbugs} ....

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Avoiding false positive data-race candidates}
\label{sec:eval-falsepos}
% Though we mechanically verify whether each data race candidate leads to a bug, each new PP can increase combinatorially..... obviously wish to avoid...

We also counted the number of malloc-recycle false positives that \landslide~suppressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Finding nondeterministic data-race candidates}
\label{sec:eval-falseneg}
Some memory accesses may be hidden in a control flow path that requires a nondeterministic preemption to be executed.
In such cases, a single-pass dynamic data-race detector
%could not achieve the coverage necessary
would fail
to identify a racing access pair as a candidate to begin with.
%
We counted how many such data-races, used as PPs, led to \quicksand~finding new bugs,
thereby making them {\em false negatives} of the single-pass approach.
% TODO: Put a figure here giving an example of where e.g. a data race only shows up during the contention path of a mutex.
We classified each data-race candidate according to whether \landslide~had reported them during the first interleaving, before any backtracking or preempting: if so, they were {\em deterministic data races} (hence could be found by single-pass).

To ensure a fair comparison, we disabled \landslide's {\em false-positive}-avoidance techniques during this experiment.
For example, we reported malloc-recycle data races during the first interleaving as {\em deterministic}, as a single-pass analysis must,
rather than waiting until future interleavings to confirm them (as explained in \sect{\ref{sec:recycle}}).

				% TODO: Argue:
				% It is fair to compare multiple pass DR-analysis under Landslide against just a single execution because prior work DR detectors, being not integrated with a MC, are not intended to uncover different results under subsequent runs.
				% Define a "stress tester" as a class of bug detectors where they are intended to [...]
				% Maybe say: Do there exist any stress-tester DR detectors where they are intended to produce new results under reruns?

% TODO: restructure this paragraph to account for RaceFuzzer/Portend. Be like, "Yes they explore multiple schedules, but only AFTER finding a race." And/or make this argument in prior work
One might also wonder: Why is it fair to compare the data race bugs \quicksand~finds (10 CPU-hours)
against the candidates found by a prior work's single execution (less than a minute)?
We argue this comparison is meaningful because prior work data-race tools, being not integrated with a MC,
are not intended to uncover different results under subsequent runs.
One could run a data-race tool repeatedly for 10 CPU-hours, but the advantage of stateless MC over stress testing is already well-understood.
% TODO TODO TODO get a big citation for teh above sentence.


% Figure out concretely what the data race tricks are that we do, so we can claim them as contributions in the paper. Then ACTUALLY EVALUATE THEM.
%         - Speculative DR PPs.
%                 Not a heuristic, rather how to make it work at all to begin with.
%                 (Cite MS thesis, claim on backwards explorating finding bugs faster)
%         - Free/re-malloc to eliminate some false positives. See #193.
%                 Measure how many false positives are eliminated.
%                 Check, ofc, to make ABSOLUTE SURE, that no bugs missed w/ this trick.
%                         If there are, it could be because of the implementation
%                         bug described in #193.
%         - Using tid/last_call filtering because whole stack traces are too expensive.
%                 Moderately optional, 1st priority since theoretically interesting:
%                 Turn on/off and measure how resulting DR bug #s change.
%         - Optional: Reprioritizing DRs based on "confirmed" / "suspected"
%                 Shouldn't be hard just make ID wrapper print "s" or "c"!
%                 Is it helpful for ID to put priorities on DR PPs?
%                         Test by inverting the priority and see if fewer buges are found.
%         // Super optional to talk about. Probably not worth the time.
%         // - "Too suspicious" (during init/destroy)
%         //      (Cite eraser, section 2.2)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Discussion and future work}
\label{sec:future}

%TODO: Run a mutex expt where "all atomic instrs" are PPs. See how many bugs are missed anyway.
% Wwe re-ran the \mxtest control experiment with \landslide~hard-coded to preempt on any atomic instruction
% (as well as on the mutex API boundaries).
% Still, this smarter configuration for SSS-MC found only 99999999999 bugs of \quicksand's 13.

{\bf Testing lock implementations.}
In \sect{\ref{sec:eval-sssmc}}, using data-race PPs compares favourably across the board to SSS-MC.
Observe in particular that in \mxtest, the control experiment found dramatically fewer bugs (just 1),
even compared to the other test cases\footnote{
% FIXME: "Aren't all lock impls assembly?" "Yes, but this one was ALL assembly."
	The one bug SSS-MC found was in a fully-assembly lock implementation. {\tt yield()}'s return value clobbered a value stored in {\tt \%eax}, which could lead to a failure after two repeated contentions. Preempting only on {\tt yield()} (in the contention loop) was sufficient.}.
%Intuitively, this is due to our control experiment being able to preempt only on the boundaries of the API which
%Though for many applications of MC, assuming a correct lock implementation is sufficient,
Though it suffices for many applications of MC to assume correctly-implemented locks,
we consider this strong evidence that any verification of low-level synchronization code must incorporate data-race PPs.

% TODO: get these numbers
{\bf Finer-grained PP subsets.}
\quicksand~was able to partially guarantee safety in {\large \bf 99999\%} of tests with large maximal state spaces.
However, in {\large \bf 1337} tests, no more than the minimal state space could be verified,
and in {\large \bf 42} tests, not even that much.
Larger state spaces often result from finer-grained locking,
which can indicate a more complicated concurrent algorithm requiring more rigorous verification than a program with a single global lock.
%Hence these corner cases are important to consider for future work.
While this work used {\tt within\_function} (\sect{\ref{sec:landslide}}) {\em statically} to restrict where PPs could arise in advance of the test,
we envision future Iterative Deepening implementations could incorporate this method to {\em dynamically} subset PPs further,
making partial verification of such large tests possible.
%enabling partial verification of such large tests. % if need space

{\bf Partial verification.}
We are not the first to provide a partial verification guarantee when timing out on too-large state spaces (\sect{\ref{sec:eval-sssmc}}).
While we guarantee safety when preempted on certain combinations of PPs,
CHESS
guarantees safety under no more than a certain number of preemptions \cite{chess-icb}.
%according to the maximum bound reached in the time limit.
We imagine these two guarantees could be each be useful to developers in different scenarios,
and are presently working to combine the two approaches to provide both at once.
One benefit of our technique is that {\tt within\_function}-based Iterative Deepening (discussed above)
would enable expert developers to configure custom subsets of PPs they are most interested in verifying,
according to which modules of a codebase they wish to test.

% Future work: Add parallel DPOR so you can fill your spare CPUs when there are fewer than the max number of jobs.
