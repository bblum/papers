%
% LaTeX template for prepartion of submissions to PLDI'15
%
% Requires temporary version of sigplanconf style file provided on
% PLDI'15 web site.
%
\documentclass[pldi]{sigplanconf-pldi15}

%
% the following standard packages may be helpful, but are not required
%
\usepackage{SIunits}            % typset units correctly
\usepackage{courier}            % standard fixed width font
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{amsthm}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{algpseudocode}
\usepackage{graphicx}
%\usepackage{setspace}
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI
%\doublespacing


\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}

\begin{document}

%
% any author declaration will be ignored  when using 'plid' option (for double blind review)
%

%\newcommand\landslide{\textsc{Landslide}} % Don't mention landslide in the proof.
\newcommand\quicksand{\textsc{Quicksand}}
\newcommand\simics{\textsc{Simics}}
\newcommand{\sect}[1]{\S #1}
\newcommand\hilight[2]{\color{#1}#2\color{black}}
\definecolor{olivegreen}{RGB}{0,127,0}
\definecolor{brickred}{RGB}{192,0,0}

%\title{Soundness Proof for Eliminating Malloc-Recycle Data Races in Stateless Model Checking}
\title{Soundness Proofs for Iterative Deepening}
\authorinfo{Ben Blum}{Carnegie Mellon University}{bblum@cs.cmu.edu}

\maketitle
\begin{abstract}
In our paper we show the effectiveness of combining dynamic data-race detection \cite{eraser,hybriddatarace} with stateless model checking \cite{verisoft,dpor}.
Our approach involves adding new state spaces to explore each time a new data-race candidate is found.
This document presents two proofs concerning the technique.

First, we prove that given enough time, Iterative Deepening will converge to the same degree of schedule coverage that would be provided by a naive MC preempting on every single instruction.
In other words, when Iterative Deepening finishes generating new data-race PPs, and completes all associated state spaces,
it serves as a full verification of {\em all possible} thread schedules of the given test case.

Second, we prove the soundness of the {\em malloc-recycle} false-positive-elimination tactic we discuss in the paper.
Despite many powerful reduction techniques, the state spaces are still exponentially sized,
so any way to avoid exploring some in advance is obviously beneficial.
We prove that for the special case of {\em malloc-recycle} false positives, it is safe to eliminate these immediately upon discovering them, without bothering to explore their associated state spaces.


This document is supplemental material to our main paper; we assume the reader is already familiar with our motivation and terminology.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Definitions}

\subsection{Stateless model checking terms}

\begin{definition}[Transition]
A sequence of instructions executed by the program between two preemption points (PPs).
\label{def:transition}
\end{definition}
Our model checker provides the invariant that each transition's instructions are associated with exactly one thread. (That is, the set of PPs always includes all thread switches.)

\begin{definition}[Must-happen-before (MHB)]
	Given two transitions $A$ and $B$, we say $A$ MHB $B$ if $B$ cannot be reordered to occur before $A$.
\end{definition}
All transitions of the same thread are trivially MHB-related.
Two transitions $A$ and $B$ of different threads MHB if some synchronization event in $A$ causes $B$ to become runnable while it was previously blocked. Such synchronization events include {\tt thread\_create}, {\tt cond\_signal}, {\tt sem\_post}, but {\em not} {\tt mutex\_lock} or {\tt mutex\_unlock}.
% FIXME: Address concern about using blocking sync primitives, eg sem or rwlock, in a mutex like fashion.
% FIXME: Even though MHB(T1_0, T1_1) and MHB(T1_1, T2_0), then still not necessarily MHB(T1_0, T2_0).

Note how our {\em must}-happen-before relation differs from the conventional definition of happens-before (``observed to happen before'') \cite{lamport-clocks}.
Our use of MHB matches the ``limited happens-before'' used in \cite{hybriddatarace} and \cite{tsan};
the advantage of this over pure-happens-before detectors in producing fewer false negatives is well-argued in those prior works\footnote{
Because pure-HB data race detectors avoid false positives altogether, they would have no trouble avoiding our malloc-recycle false positives.
However, as prior work has shown, they miss many other bugs involving unprotected variables accessed alternately before and after mutex-protected critical sections.
Indeed, because most concurrent malloc implementations are protected by a lock,
our malloc-recycle false positives are indistinguishable from such false negatives under pure-HB.
}.

\begin{definition}[Shared memory conflict]
A pair of memory accesses between two threads to the same address where at least one of them is a write.
\end{definition}
Intuitively, the behaviour of a program could change by reordering two transitions only if they contain a memory conflict.

\begin{definition}[Dynamic Partial Order Reduction (DPOR)]
	A state-space search algorithm for stateless model checkers,
	guaranteed to reorder transitions of two threads
	iff they have a shared memory conflict and are not related by MHB \cite{dpor}.
	\label{def:dpor}
\end{definition}

\subsection{Data race and other memory terms}

\begin{definition}[Data race]
A shared memory conflict where furthermore:
\begin{itemize}
	\item The intersection of both threads' locksets is empty (i.e., the threads do not hold the same lock during each access), and
	\item The threads' transitions are not related by MHB.
\end{itemize}
\end{definition}

The same as in the paper, we distinguish between data-race {\em candidates} and data-race {\em bugs}.
In this proof we are concerned solely with candidates, and judging whether they are true data races or false positives.
It is up to \quicksand, outside the scope of this proof, to decide whether true data races are benign or buggy.

\begin{definition}[Reachable data race]
A data race candidate which will be identified by a MC configured to preempt only on locking API boundaries,
or transitively also configured to use data-race PPs of other (already) reachable data races.
\end{definition}

%To prove a theory, generally sized super mario bros is not possible to complete, there is two step, one step for each F.
To prove Iterative Deepening's convergence to full coverage,
we are concerned with showing that any data race PP necessary to expose a bug will be reachable.

\begin{definition}[False positive data race]
	An apparent data race that cannot be observed in the opposite order from what was actually executed.
\end{definition}

False positives are caused when some data dependency based on some other shared state, invisible to the data-race analysis,
changes some variable values when the threads are reordered, such that the memory addresses no longer collide.

\begin{definition}[Malloc-recycle data race]
	A data race where the address is contained in some heap-allocated memory, and between the two accesses, that memory was passed to free() and returned again by a subsequent malloc().
\end{definition}

Figures~\ref{fig:recycle} and \ref{fig:recycle-bug} show an example.
In the case of malloc-recycle false positives, the allocation heap is the ``other shared state'' mentioned in the previous definition, and malloc's return value is the variable value that changed.

\begin{definition}[Use after free]
	Any read or write to heap memory which was once allocated, but no longer is.
\end{definition}

These can immediately be identified as failures by a MC which tracks allocation state.
%Most commonly this refers to accesses to a region already freed, but for brevity we also include
% TODO: need heap block overrun?

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& \multicolumn{2}{c}{\texttt{struct x \{ int foo; int baz; \} *x;}} \\
	& \multicolumn{2}{c}{\texttt{struct y \{ int bar; \} *y;~~~~~~~~~~}} \\
	\\
	& Thread 1 & Thread 2 \\
	1 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	2 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	3 & & \texttt{// x's memory recycled} \\
	4 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	5 & & \texttt{// ...initialize...}\\
	6 & & \texttt{publish(y);} \\
	7 & & \texttt{\hilight{brickred}{y->bar = ...;}} \\
\end{tabular}
\caption{False-positive malloc-recycle pattern. This is the common case for which we avoid creating new state spaces.}
\label{fig:recycle}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\
	1 & \texttt{publish(x1);} & \\
	2 & & \texttt{x2 = get\_published\_x();} \\
	3 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	4 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	5 & & \texttt{// x's memory recycled} \\
	6 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	7 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{Adversarial program which fits the malloc-recycle pattern, but nevertheless contains a true race.}
\label{fig:recycle-bug}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\
	1 & \texttt{publish(x1);} & \\
	2 & & \texttt{x2 = get\_published\_x();} \\
	3 & & \texttt{// x not free, so malloc's} \\
	4 & & \texttt{// return value changes!} \\
	5 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	6 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	7 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	8 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
\caption{Goal interleaving, which reorders the adversarial program's threads away from the pattern, while the data race remains.}
\label{fig:recycle-goal}
\end{figure}

\section{Intuition}

In this section we provide (hopefully) intuitive summaries of both our proof goals,
for readers who might be satisfied without necessarily following the proofs' internal structure.

{\bf Intuition for Iterative Deepening convergence.}
In summary, we are proving that when Iterative Deepening finishes saturating the set of available data-race PPs,
that set represents every single code location where a preemption could possibly affect the program's behaviour,
and that completing the associated state spaces is as strong of a verification as testing all possible thread interleavings under any preemptions anywhere.
Some data-races may be hidden in control-flow paths which could only be executed after preempting on a different data-race,
but the technique's iterative nature will eventually find it.
On the other hand, relying on the soundness of DPOR, preempting on an instruction which is neither a data-race or sync API boundary cannot affect the program's behaviour.

{\bf Intuition for malloc-recycle soundness.}
In summary, we are proving that if a malloc-recycle-pattern data race is a true race, rather than a false positive,
then DPOR is guaranteed to ``reorder away the free and re-malloc''.
In other words, DPOR's exploration will eventually interleave threads in such a way that the malloc-recycle pattern will disappear,
while the access pair remains for the data-race detector to find, as shown in Figure~\ref{fig:recycle-goal}.
Hence, in the same state space where the malloc-recycle data race was found, if it's a true race, the same race will also appear without the recycle pattern.
So if that race hides a failure bug (assertion, segfault, etc.), Iterative Deepening will still be led to the necessary preemption point to find that bug.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Assumptions}

This section documents the assumptions we make about the concurrency model, language model, and test environment,
and discusses some limitations that may arise from these assumptions.

\subsection{Assumptions for both proofs}

{\bf Maximal state space.}
We assume the model checker has all static/hard-coded PPs enabled during this state space exploration,
and that we are not limited by a pressing CPU budget.
Further, we assume that the static PPs include all lock/unlock/trylock operations on mutexes (or whatever other low-level locks are used) and also all higher-level sync primitives which can cause HB (either directly, or because they are built on top of mutexes).

Using only a subset of PPs or aborting early due to time-out could each ruin our ability to reach the goal interleaving or goal state space.
However, Iterative Deepening aims to test the most important interleavings with the time available,
so in the case of not enough time, our point here is that continuing the current state space fits that goal best.
%An alternate approach would be to accept the malloc-recycle PPs, giving them the lowest possible priority, just in case any CPUs would otherwise be idle with not enough jobs to run.
%In that case, our point is to justify deprioritizing those jobs so aggressively.

%{\bf Low overhead.}
%We assume the model checker can identify malloc-recycle data races with little or no overhead beyond what's already associated with data race detection.
%Our MC already tracks the heap state, so we implement this check for free with a simple generation counter.

{\bf Shared memory thread communication.}
We assume that the only way for two threads' transitions to affect each other's behaviour, should they be reordered,
is through either shared memory or a correctly-instrumented sync API. Both DPOR and data-race detec

{\bf Schedule nondeterminism only.}
We discount the possibility of other types of nondeterminism,
such as program input nondeterminism (including randomness/timestamps) or
store-buffer nondeterminism on weak-memory architectures.
We refer the reader to \cite{klee,portend} for related work on the former, and to \cite{tsopso} for the latter.

\subsection{Assumptions for Iterative Deepening convergence}

{\bf Locks are correct.}
Because hybrid data-race detection uses lockset analysis to conclude that many access pairs could not possibly race,
we assume that no preemption during a lock-protected critical section could cause a contending thread to make meaningful progress.
This extends to use of disabling/enabling interrupts during kernel-space testing, which we model as a single global lock,
although we do not model RCU.
Our data-race analysis also models the behaviour of r/w locks and 1-initialized semaphores (the latter heuristically), although this is tangential to the proof.

Should the user wish to verify these properties of locks,
they could either run a locking test separately (as we suggest in our paper),
which would cheaply test the locks to an extent limited by the separate test case;
or they could remove the data-race analysis's lockset tracking,
which would expensively test all the main program's required locking properties in tandem with the program itself.

\subsection{Assumptions for malloc-recycle soundness}

{\bf Malloc is a magic black box.}
We assume the malloc implementation is correct (e.g., it won't double-allocate blocks), although we don't assume any implementation details such as a tendency to reuse blocks or allocate adjacent ones.
%particular allocation pattern regarding adjacency/coalescing/reuse.
In fact, in our experiments we instruct our MC to ignore all potential PPs which might be inserted on malloc's internal mutex;
in essence treating it as a ``magic primitive'', because we are not interested in verifying its implementation.
Furthermore, we configured DPOR to ignore the internal heap metadata accesses
when tracking shared memory conflicts to achieve greater state space reduction
(i.e., if the only consequence of reordering two transitions is malloc returning different addresses, we consider them independent).
%for the purpose of avoiding malloc-only shared memory conflicts.
This is not without consequences; see section~\ref{sec:owned}.

{\bf Sharing heap addresses.}
Finally, we assume that the only way the program can obtain heap addresses is through the return value of malloc().
Because we are testing C programs, any bizarre violations of this assumption are technically possible,
but should you wish to check for bugs like this,
%we would recommend a data-flow analysis which is much cheaper than model checking anyway.
symbolic execution \cite{klee} would be more appropriate.

For Section \ref{sec:proof}, we further assume a malloced block's address cannot be obtained through arithmetic on the address of a different block; in Section \ref{sec:owned} we show how to account for this case by relaxing the previous ``black box'' assumption.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of Iterative Deepening convergence}

% TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Proof of malloc-recycle soundness}
\label{sec:proof}

We seek to prove that ignoring malloc-recycle data races cannot cause DPOR + Iterative Deepening to miss a bug that could be found by using the malloc-recycle race as a PP. Hence, our soundness statement is as follows:

\begin{theorem}[Soundness of eliminating malloc-recycle races]
	If a malloc-recycle data race is not a false positive, DPOR will reorder threads such that either the same accesses will still race without fitting the malloc-recycle pattern, or a use-after-free bug will be reported immediately.
\end{theorem}

Though Figures~\ref{fig:recycle-bug} and \ref{fig:recycle-goal} show example programs, they do not capture all possible cases of how a true data race can fit the malloc-recycle pattern.
We proceed by establishing what must be true of any such program, then casing on the ambiguous possibilities, and showing that PPs will exist where we need them to reorder the threads.

For certain, there must be an access in one thread, followed by a free and malloc (we'll call them ``middle free'' and ``middle malloc''), each possibly from either thread, followed by an access from the other.
If the data race is not a false positive, then the second access must not change locations based on the middle malloc's return value.
WLOG, we say that thread 1 (T1) does the first access, called {\tt x1}, and thread 2 (T2) does the second, {\tt x2}.

\begin{lemma}
	If DPOR will reorder {\tt x2} to before {\tt x1}, and the location of access {\tt x2} doesn't change,
	then a non-malloc-recycle data race or a use-after-free bug will be identified.
	\label{lem:reorder}
\end{lemma}
\begin{proof}
By case on which threads the middle free and middle malloc came from.
\begin{itemize}
	\item T1 free, T2 malloc (as shown in Figure~\ref{fig:recycle-bug}). The malloc will go with {\tt x2} to before the free, and because the allocation of concern has not been freed yet, will return a different value. Hence {\tt x1} and {\tt x2} will be in the same allocation; hence the race is not malloc-recycle anymore.
	\item T1 free, T1 malloc. Same as above, but the malloc does not move. The middle malloc will still recycle the memory, but {\tt x2} now occurs before then, being in the same, older, allocation.
	\item T2 free, T2 malloc. Both the free and re-malloc will occur before either {\tt x1} or {\tt x2}. The memory will be recycled and both accesses will appear to be in the later allocation.
	\item T2 free, T1 malloc. The free gets reordered earlier, the malloc stays put, and the accesses go in between. This will be a use-after-free bug.
\end{itemize}
If either the middle free or middle malloc came from a third thread, the case is the same as if it belonged to T1.
\end{proof}

The keen reader might ask here, what if T1 contains some extra spurious malloc calls (not related to {\tt x1}) that affect what T2's malloc returns after being reordered?
However, these could at best either cause {\tt x}'s memory to be recycled slightly differently (not affecting the proof), or not at all (which simply changes some cases to be immediate use-after-frees).
%If such a malloc could affect {\tt x2}, the data race would be a false positive after all.
In general, adding spurious mallocs that could affect {\tt x1} or {\tt x2} could only convert the program back into a false-positive scenario;
and adding more spurious synchronization events could only make it more easy to find PPs we need to trigger the reordering.
So, WLOG, we say that the only relevant events are the ones we mention explicitly.

By our last assumption, there must also be an ``original malloc'' which allocated the heap block to begin with.
We must ask, which thread did the malloc which returned {\tt x1}'s address in the first place?
Because of our last assumption, we know that the other thread must obtain that address through some communication mechanism (which we'll reason about later).

\subsection{T2 originally malloced x}

\begin{lemma}[Greedo]
	If T2 originally malloced the block containing {\tt x}, DPOR will reorder the threads to uncover a non-malloc-recycle race or a use-after-free bug.
	\label{lem:greedo}
\end{lemma}
\begin{proof}
Because T1 had the first access, there was a thread switch between the original malloc and {\tt x1}, as well as between {\tt x1} and {\tt x2}. By Definition~\ref{def:transition}, each switch will be a PP.
By Definition~\ref{def:dpor}, DPOR will reorder {\tt x2} to before {\tt x1},
and because T1 is not involved in the logic determining {\tt x2}, the access's location stays the same.
Lemma~\ref{lem:reorder} finishes.
\end{proof}

This lemma also applies if a third thread was responsible for this malloc, as there would still be a thread switch in the same spot.

\subsection{T1 originally malloced x}

\begin{lemma}[Han]
	If T1 originally malloced the block containing {\tt x}, DPOR will reorder the threads to uncover a non-malloc-recycle race or a use-after-free bug.
	\label{lem:han}
\end{lemma}

\begin{proof}
We must guarantee there will be a PP during T1 before its {\tt x1} access, but after whatever action it took to communicate the heap address to T2\footnote{
Note why we assert the publish action must come before {\tt x1}: otherwise, T2 couldn't be reordered to race {\tt x2} with {\tt x1} before T1 communicated the address, and it would be a false positive after all.}.

%\begin{figure}[t]
%	\small
%\begin{tabular}{rll}
%	& Thread 1 & Thread 2 \\
%	0 & \texttt{lock();} & \\
%	1 & \texttt{publish(x1);} & \\
%	3 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
%	4 & \texttt{\hilight{olivegreen}{free}(x1);} \\
%	0 & \texttt{unlock();} & \\
%	0 & & \texttt{lock();} \\
%	2 & & \texttt{x2 = get\_published\_x();} \\
%	0 & & \texttt{unlock();} \\
%	5 & & \texttt{// x's memory recycled} \\
%	6 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
%	7 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
%\end{tabular}
%\caption{If the locksets are the same in T1 during publish and x1, you can't avoid the data race on publish+get.}
%\label{fig:recycle-bug}
%\end{figure}

We ask: was there a synchronization event (unlock or message-pass) between the publish action and {\tt x1}?
If so, then by the maximal state space assumption, the event will be a PP, and we are done.
If not, then  T1's lockset during publish will be the same as the lockset during {\tt x1} (and the fact that there is no MHB cannot change).
For T1's publish to reach T2, they must access the same memory ({\em outside} of the block containing {\tt x1}; T2 doesn't have that yet),
which we'll call {\tt p}.
By the previous two statements, {\tt p} must be a data race of its own.
%(whether by writing {\tt x}'s address directly into the shared memory, or by sharing a pointer to a data structure containing {\tt x} -- doesn't matter which)

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\

	%1 & \texttt{p1 = \hilight{olivegreen}{malloc}(...);} & \\
	1 & \texttt{\hilight{brickred}{p1->ptr = x1;}} & \\
	2 & \texttt{publish(p1);} & \\
	3 & \texttt{\hilight{olivegreen}{free}(p1);} & \\
	4 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	5 & \texttt{\hilight{olivegreen}{free}(x1);} \\


	6 & & \texttt{p2 = get\_published\_p();} \\
	7 & & \texttt{// p's memory recycled} \\
	8 & & \texttt{q = \hilight{olivegreen}{malloc}(sizeof *q);} \\
	9 & & \texttt{\hilight{brickred}{x2 = p2->ptr;}} \\


	10 & & \texttt{// x's memory recycled} \\
	11 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	12 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{If the accesses used to publish {\tt x}'s address are a data race, their PPs may also be eliminated under the malloc-recycle pattern. Induction on the pointer structure leading to {\tt x} handles this case.}
\label{fig:induction}
\end{figure}

Because it's possible for {\tt p} to {\em also} be a malloc-recycle data race,
as shown in Figure~\ref{fig:induction},
we do not necessarily get the PP for free here.
In this case we need to prove that DPOR will likewise reorder any intermediate malloc-recycle pattern to generate the PP we need\footnote{
In \quicksand, data race PPs are not used immediately, but rather generate new state spaces to explore in the future. Anyway, under the maximal state space assumption, we will get to it eventually.}.
We handle this with induction on T2's pointer chain leading to {\tt x}.

\newcommand\publish[1]{{\tt p}$_{#1}$}
\begin{itemize}
	\item For the base case, the publish location \publish{0} is either in global memory, or shared directly using synchronization.
		Non-heap memory data races are not subject to the malloc-recycle pattern, so will always get a data-race PP,
		and use of synchronization always gets a PP in the maximal state space.
	\item For the inductive step, a pointer \publish{n} is published in some heap memory \publish{n-1}\texttt{->ptr},
		and we assume that however \publish{n-1} is shared to T2,
		there will be a PP there
		% where \publish{n-1} is shared
		sufficient to make the \publish{n-1}\texttt{->ptr} access not malloc-recycle after DPOR.
		%DPOR will test some interleaving in which T1's
		Hence a data race PP will be generated on the \publish{n-1}\texttt{->ptr} access.
		Hence by Definition~\ref{def:dpor} and Lemma~\ref{lem:reorder},
		DPOR will reorder T1's and T2's subsequent accesses to \publish{n} sufficiently to place a PP on them.
		% i.e., so that the p_n access no longer appears to be malloc-recycle.
\end{itemize}

Hence, even if the accesses by which T1 shares {\tt x} with T2 appear in a different malloc-recycle pattern,
a PP will be identified on the publish location {\tt p}.
Hence by Definition~\ref{def:dpor} DPOR will reorder T2's execution to just after the publish action.
% XXX: Landslide does not do this! It only puts a PP immediately *before* the access. That's incompatible with this, which needs you to put one both before and after.
As long as T2's execution occurs after the publish, it will receive the same value for its {\tt x2}, so the location of the data race does not change.
Lemma~\ref{lem:reorder} finishes.
%Hence for any type of publish action, there will be a PP between it and {\tt x1}, hence DPOR will reorder {\tt x2} to before {\tt x1} (without changing the address at which {\tt x2} occurs), and Lemma~\ref{lem:reorder} finishes.


\end{proof}


%We will also say that T1 does the initial malloc which first returns {\tt x1}'s address; although certainly T2 or even a third thread could have malloced it, that would involve a thread switch to thread 1, and hence a PP by Definition~\ref{def:transition}.
%As we will see, the main challenge of the proof is showing that enough PPs will exist at appropriate points, so adding more PPs by considering other threads for the initial malloc can only make the proof easier.

\subsection{Conclusion}

\setcounter{theorem}{0}
\begin{theorem}[Soundness of eliminating malloc-recycle races]
	If a malloc-recycle data race is not a false positive, DPOR will reorder threads such that either the same accesses will still race without fitting the malloc-recycle pattern, or a use-after-free bug will be reported immediately.
\end{theorem}
\begin{proof}
	Between Lemmas \ref{lem:greedo} and \ref{lem:han}, all cases of possible program structure are covered.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Heap overruns}
\label{sec:owned}

Actually, there is another way to share the allocation's address without T1 and T2 communicating outside of {\tt x1}/{\tt x2}: one thread overruns a {\em different} heap block which happened to be adjacent to the one containing {\tt x1}/{\tt x2} (call them the ``neighbour block'' and ``real block'' respectively).
Figure~\ref{fig:overrun} shows an example.
Heap overrun bugs are quite serious \cite{eternal-war}, so we do not wish to exclude them from our model.
So we must also consider the cases where T1 mallocs the real block and T2 overflows a neighbour, and vice versa.


\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\
	1 & & \texttt{z = malloc(42);} \\
	2 & & \texttt{// TODO bounds check??} \\
	3 & & \texttt{x2 = \&z[50];} \\
	4 & \texttt{x1 = malloc(...);} & \\
	5 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	6 & \texttt{\hilight{olivegreen}{free}(x1);} \\
	7 & & \texttt{// x's memory recycled} \\
	8 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	9 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
\end{tabular}
\caption{Final possibility for how T2 can share T1's allocation address, and probably a security vulnerability to boot!}
\label{fig:overrun}
\end{figure}

\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\
	1 & & \texttt{z = malloc(42);} \\
	2 & & \texttt{// TODO bounds check??} \\
	3 & & \texttt{x2 = \&z[50];} \\
	4 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	5 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	6 & \texttt{x1 = malloc(...);} & \\
	7 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	8 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
\caption{Without a PP between lines 4 and 5 of Figure~\ref{fig:overrun}, this is the only alternate interleaving DPOR would explore. Because the mallocs have been reordered, they may no longer collide, which wrongly appears to be a false positive.}
\label{fig:overrun-notenough}
\end{figure}


\begin{figure}[t]
	\small
\begin{tabular}{rll}
	& Thread 1 & Thread 2 \\
	1 & & \texttt{z = malloc(42);} \\
	2 & & \texttt{// TODO bounds check??} \\
	3 & & \texttt{x2 = \&z[50];} \\
	4 & \texttt{x1 = malloc(...);} & \\
	5 & & \texttt{y~=~\hilight{olivegreen}{malloc}(sizeof *y);} \\
	6 & & \texttt{\hilight{brickred}{x2->foo = ...;}} \\
	7 & \texttt{\hilight{brickred}{x1->foo = ...;}} & \\
	8 & \texttt{\hilight{olivegreen}{free}(x1);} \\
\end{tabular}
\caption{Goal interleaving of Figure~\ref{fig:overrun}. To ensure collision, the sequence of malloc calls which produce {\tt x1} and {\tt x2} must not be disrupted.}
\label{fig:overrun-goal}
\end{figure}

% TODO: Replace this with some real discussion.

%{\bf Being Owned.}
%The second best thing I can think to do is say, put "speculative" PPs on the end of malloc, and enable them only when that malloc is involved in a FRM situtation. Then, measure among all FRM sitations (gs12642/landslide/pldi/p2-live/frms) how much bigger the state spaces get (and whether that affects our bugfinding ability/cputimes).
%
%
%The first best thing I can think to do is to argue that this case doesn't disrupt soundness because the original model we were working with was already unsound. If the other interleaving ("the best dpor could find") occurred first, dpor would not identify a memory conflict at all, and would not reorder to find the original interleaving (with the FRM false pos) at all. this is because dpor ignores malloc's internal accesses from the shm conflict relation.

what we are trying to prove is that, if we START WITH a sound dpor, and add this FRM-elimination, you end up with a sound dpor. BUT, the original one was not sound with respect to this bug to begin with.

then you can simply point out that if you want dpor to be sound wrt this bug, you have to put a PP on the end of malloc (other spots, beginning of malloc and either end of free, are not needed), and also make the shm conflix relation account for the way malloc reordering can change the address collision issue.


\begin{lemma}
	asdfa
	\label{lem:leia} % "there is another"
\end{lemma}

% TODO
% We suspect that DPOR + pure-HB will always eventually find all data races uncovered by MHB.
% for the purpose of iterative deepening, MHB is
% better because it will find the data races much sooner, rather than requiring you
% to explore the maximal state space before the race can be uncovered


%\subsection{Malloc with internal mutexes}
%
%First we assume that the malloc implementation is guarded by
%
%
%Mario man is very very hunger from not having enough plumming jobs, so his Quest for Eat and Dollars.
%
%
%\subsection{Malloc as a magic primitive}
%
%Actually, regardless of malloc's internal synchronization details, in virtually all cases we wish to avoid unnecessary PPs on its boundaries during MC, because unlike other uses of mutexes, it doesn't indicate any concurrent behaviour is going on.
%Much like the {\tt mx\_test} in our paper, the malloc implementation could itself be tested for concurrency bugs, but in all other cases,
%we drastically reduce the state space size by assuming it is correct and avoiding PPs on it, treating it instead as a ``magic primitive''.
%
%Hence, we provide another proof that does not require
%
%%\cite{landslide} % uncomment post-double-blind review
%
%This spells QED so we are done.

\section{Acknowledgements}

{\em removed for double blind review}
%Thanks to Michael J. ``Sully'' Sullivan and Carlo Angiuli, who double-checked the proof and helped with the flow of explanation.

\bibliographystyle{abbrvnat}
\bibliography{citations}{}
\end{document}
